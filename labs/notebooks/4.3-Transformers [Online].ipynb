{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# 4.3 - Transformers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al igual que suced√≠a con las [RNN](https://pytorch.org/docs/stable/nn.html#recurrent-layers) (LSTM y GRU) o la Attention, en pytorch tambi√©n tendremos a nuestra disposici√≥n capas ya existentes que nos ayudan a la creaci√≥n de un [Transformer](https://pytorch.org/docs/stable/nn.html#transformer-layers). \n",
        "\n",
        "Estas capas son:\n",
        "\n",
        "* **`nn.TransformerEncoderLayer`**: Esta capa representa una √∫nica capa del encoder de un transformer.\n",
        "* **`nn.TransformerDecoderLayer`**: Similar a la capa del encoder, esta capa representa una √∫nica capa del decoder del transformer.\n",
        "* **`nn.TransformerEncoder`**: Esta clase agrupa m√∫ltiples capas de `nn.TransformerEncoderLayer`, formando el encoder completo del transformer.\n",
        "* **`nn.TransformerDecoder`**: Esta clase agrupa m√∫ltiples capas de `nn.TransformerDecoderLayer`, formando el decoder completo del transformer.\n",
        "\n",
        "Adem√°s, existe una capa que facilita todo el proceso al reunir todas las piezas en una sola estructura:\n",
        "\n",
        "* **`nn.Transformer`**: Esta clase combina el encoder y el decoder, formando el modelo completo de transformer. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3.1 - ü§ó Hugging Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta pr√°ctica, al igual que en las anteriores, podr√≠amos crear un **Transformer** para resolver un problema relacionado con secuencias (predecir la siguiente palabra, clasificar textos, ...). \n",
        "\n",
        "En vez de realizar esto, vamos a utilizar **modelos ya entrenados** disponibles en [**Hugging Face**](https://huggingface.co/models).\n",
        "\n",
        "Hugging Face es una comunidad que proporciona acceso a una amplia variedad de modelos. Para utilizarlos en python, simplemente tendremos que instalar la librer√≠a `transformers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A pesar de su nombre, esta no se limita √∫nicamente a modelos Transformer, la biblioteca tambi√©n incluye, entre otros:\n",
        "\n",
        "1. **Modelos de procesamiento de lenguaje natural (NLP)**\n",
        "2. **Modelos de visi√≥n por computador (CV)**\n",
        "3. **Modelos de multimodalidad**\n",
        "4. **Modelos de audio**\n",
        "5. **Otros**\n",
        "\n",
        "Esta nos permite, adem√°s de utilizar modelos pre-entrenados para realizar inferencia, realizar transfer-learning y fine-tunning de estos con el objetivo de adaptarlos mejor a nuestra tarea concreta.\n",
        "\n",
        "### ¬øC√≥mo utilizar un modelo ya creado?\n",
        "\n",
        "Para saber como utilizar un modelo dentro de la librer√≠a transformers, simplemente tendremos que irnos a la parte superior derecha de su web y hacer click en `Use this model > Transformers`:\n",
        "\n",
        "![image.png](https://i.imgur.com/d0ma8O2.png)\n",
        "\n",
        "Esto nos indica como cargar el modelo, pero normalmente no detalla como realizar inferencia (probar el modelo). \n",
        "Para ello es recomendable acceder a la ayuda de la librer√≠a Transformers, buscar la tarea que pretendamos realizar y a la secci√≥n de inferencia, por ejemplo, para modelos que resumen texto: https://huggingface.co/docs/transformers/tasks/summarization#inference\n",
        "\n",
        "\n",
        "Como ver√°s, la librer√≠a Transformers de Hugging Face ofrece dos enfoques principales para trabajar con modelos: **pipelines** y **clases espec√≠ficas** para cada modelo.\n",
        "\n",
        "Aqu√≠ est√°n las diferencias clave entre ambos:\n",
        "\n",
        "#### *Pipelines*\n",
        "El uso de pipelines est√° dise√±ado para facilitar la vida del usuario. Proporcionan una interfaz de alto nivel que oculta gran parte de la complejidad del modelo subyacente y su preprocesamiento.\n",
        "\n",
        "A continuaci√≥n ver√°s como se utiliza un [modelo](https://huggingface.co/timpal0l/mdeberta-v3-base-squad2) entrenado para responder preguntas sobre un texto dado:\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"timpal0l/mdeberta-v3-base-squad2\")\n",
        "question_answerer(question=question, context=context)\n",
        "```\n",
        "\n",
        "#### *Clases*\n",
        "\n",
        "Utilizando las clases pertinentes, tendremos m√°s control sobre todos los pasos necesarios para realizar la inferencia, lo que tambi√©n a√±ade complejidad.\n",
        "\n",
        "A continuaci√≥n ver√°s como se utiliza el modelo anterior, pero utilizando las clases pertinentes.\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"timpal0l/mdeberta-v3-base-squad2\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"timpal0l/mdeberta-v3-base-squad2\")\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)\n",
        "```\n",
        "\n",
        "<hr>\n",
        "\n",
        "A continuaci√≥n utilizaremos diversos modelos disponibles en la web basados en Transformers para realizar diferentes tareas relacionadas con texto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Ejemplo 1:** Text summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este ejemplo utilizaremos el modelo **BERT2BERT** para resumen de textos en espa√±ol. Est√° basado en el modelo BERT (Bidirectional Encoder Representations from Transformers) y se ha afinado espec√≠ficamente para esta tarea. \n",
        "\n",
        "Fue entrenado utilizando el conjunto de datos MLSUM, que incluye m√°s de 1.5 millones de pares de art√≠culos y res√∫menes en varios idiomas, incluyendo espa√±ol. \n",
        "\n",
        "Puedes encontrar m√°s informaci√≥n y detalles sobre su uso [aqu√≠](https://huggingface.co/mrm8488/bert2bert_shared-spanish-finetuned-summarization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Indicamos el checkpoint, que no es m√°s que un identificador del modelo dentro de la web de HugginFace\n",
        "model_ckpt = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\n",
        "\n",
        "# Cargamos el tokenizador que nos permitir√° introducir y codificar nuestros propios textos\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_ckpt)\n",
        "# Cargamos el modelo\n",
        "model = EncoderDecoderModel.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Para utilizar el modelo, creamos una funci√≥n (proporcionada en la web del modelo) encargada de tokenizar el texto, introducirlo en el modelo y decodificar el resultado.\n",
        "def generate_summary(text, min_length=10):\n",
        "   inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "   input_ids = inputs.input_ids.to(device)\n",
        "   attention_mask = inputs.attention_mask.to(device)\n",
        "   output = model.generate(input_ids, attention_mask=attention_mask, min_length=min_length)\n",
        "   return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Texto a analizar\n",
        "text_1 = \"√öltima lista de admitidos en la Universidad: se confirma el √©xito de los dobles grados. Ingenier√≠a Inform√°tica del Software y Matem√°ticas se confirma como la nota de corte m√°s alta. La Universidad de Oviedo ha hecho p√∫blica hoy la √∫ltima lista de adjudicaciones de plazas. La s√©ptima desde el mes de julio. Y aunque, seg√∫n lo previsto, las notas de corte han ido bajando, lo han hecho muy poco en los grados m√°s demandados. Y los que ocuparon los primeros puestos en el 'ranking' de las notas de corte en la primera lista siguen en las mismas posiciones. Se confirma, por lo tanto, el √©xito que los dobles grados tienen entre los universitarios asturianos y, por segundo a√±o consecutivo, se convierten en los t√≠tulos con notas m√°s altas de acceso. En cuanto a la nueva oferta de la Universidad de Oviedo, Criminolog√≠a se ha quedado finalmente en un 11,446; el doble grado de Derecho y Criminolog√≠a, en 11,305, y Ciencias de la Actividad F√≠sica y el Deporte, en 11,130.\"\n",
        "\n",
        "# Generamos varios res√∫menes indicando un m√≠nimo de longitud deseado.\n",
        "print(generate_summary(text_1, min_length=30))\n",
        "print(generate_summary(text_1, min_length=20))\n",
        "print(generate_summary(text_1, min_length=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Texto a analizar\n",
        "text_2 = \"As√≠ fue el primer d√≠a de universidad de los nuevos alumnos de la Polit√©cnica de Gij√≥n. Con ilusi√≥n, nervios y ganas de aprender y hacer nuevos amigos. As√≠ llegaron ayer al Edificio Polivalente de la Escuela Polit√©cnica de Ingenier√≠a (EPI) de Gij√≥n los alrededor de 200 estudiantes de nuevo ingreso que se dieron cita en la primera de las dos jornadas de acogida organizadas por la Universidad de Oviedo. De cara a este curso, cuyas clases arrancar√°n el pr√≥ximo 10 de septiembre, la EPI espera a 772 alumnos nuevos. En el anterior fueron 627, lo que supone un crecimiento del 23%. El grado en Ingenier√≠a Mec√°nica es en el que mayor subida se ha experimentado, de 149 a 214 matriculados. Las encargadas de recibir a los numerosos grupos de j√≥venes que acudieron a la EPI fueron la directora, In√©s Su√°rez, y la subdirectora de Estudiantes, Mar√≠a Placeres Gonz√°lez. Debido a la gran afluencia, se vieron obligadas a dividir a los estudiantes en dos salas. Su√°rez hizo hincapi√© durante su discurso en que ‚Äúdesde hace dos o tres a√±os estamos en una etapa en la que el empleo en Ingenier√≠a est√° casi garantizado‚Äù. ‚ÄúSon estudios que tienen su complejidad, pero con constancia y esfuerzo se sacan adelante. Abren un futuro muy amplio y prometedor‚Äù, se√±al√≥ la directora de la EPI, que resalt√≥ que ‚Äúse buscan ingenieros en todas partes y aqu√≠ tenemos mucha actividad con las empresas‚Äù. ‚ÄúAcuden a nosotros y quieren conoceros para que trabaj√©is con ellos en el futuro‚Äù, apunt√≥ Su√°rez, que puso el foco en que, recientemente, el tejido empresarial asturiano ha cambiado y ‚Äúhay muchas empresas que se est√°n estableciendo aqu√≠‚Äù. ‚ÄúUna de las razones es porque hay una Escuela potente que puede dar talento y que puede servir para que cuando termin√©is los estudios os incorpor√©is a las plantillas‚Äù, insisti√≥.\"\n",
        "\n",
        "# Generamos varios res√∫menes indicando un m√≠nimo de longitud deseado.\n",
        "print(generate_summary(text_2, min_length=30))\n",
        "print(generate_summary(text_2, min_length=20))\n",
        "print(generate_summary(text_2, min_length=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Ejemplo 2:** Part-Of-Speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El siguiente modelo utiliza una vez m√°s la arquitectura de **BERT**, pero en este caso ajustada para la tarea de Part-Of-Speech (POS).\n",
        "El objetivo de esta tarea es, b√°sicamente, identificar la funci√≥n gramatical de cada una de las palabras de un texto.\n",
        "\n",
        "En este caso, se ha entrenado para clasificar cada palabra entre 60 etiquetas POS diferentes, entre las que destacan sustantivos, verbos, adjetivos, adverbios, pronombres, preposiciones, conjunciones, y determinantes.\n",
        "\n",
        "Puedes encontrar m√°s informaci√≥n y detalles sobre su uso [aqu√≠](https://huggingface.co/mrm8488/bert-spanish-cased-finetuned-pos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Cargar el tokenizador y el modelo\n",
        "model_ckpt = \"mrm8488/bert-spanish-cased-finetuned-pos\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, use_fast=False)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Texto a analizar\n",
        "text = \"As√≠ fue el primer d√≠a de universidad de los nuevos alumnos de la Polit√©cnica de Gij√≥n. Con ilusi√≥n, nervios y ganas de aprender y hacer nuevos amigos.\"\n",
        "\n",
        "# Tokenizar el texto\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Obtener las predicciones del modelo\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Aplicar la \"softmax\" a los logits del modelo\n",
        "logits = outputs.logits\n",
        "predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Convertir las predicciones en una lista de entidades\n",
        "predicted_entities = predictions[0].numpy()\n",
        "\n",
        "# Obtener los tokens y sus correspondientes etiquetas\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "# Crear un diccionario de etiquetas\n",
        "label_dict = { \"AO\": \"Adjetivo ordinal\", \"AQ\": \"Adjetivo calificativo\", \"CC\": \"Conjunci√≥n de coordinaci√≥n\", \"CS\": \"Conjunci√≥n subordinada\", \"DA\": \"Determinante art√≠culo\", \"DD\": \"Determinante demostrativo\", \"DE\": \"Determinante exclamativo\", \"DI\": \"Determinante indefinido\", \"DN\": \"Determinante numeral\", \"DP\": \"Determinante posesivo\", \"DT\": \"Determinante interrogativo\", \"Faa\": \"Signo de apertura de admiraci√≥n\", \"Fat\": \"Signo de apertura de interrogaci√≥n\", \"Fc\": \"Signo de puntuaci√≥n: coma\", \"Fd\": \"Signo de puntuaci√≥n: dos puntos\", \"Fe\": \"Signo de puntuaci√≥n: puntos suspensivos\", \"Fg\": \"Signo de puntuaci√≥n: guion\", \"Fh\": \"Signo de puntuaci√≥n: cierre de admiraci√≥n\", \"Fia\": \"Signo de puntuaci√≥n: apertura de par√©ntesis\", \"Fit\": \"Signo de puntuaci√≥n: cierre de par√©ntesis\", \"Fp\": \"Signo de puntuaci√≥n: punto\", \"Fpa\": \"Signo de puntuaci√≥n: apertura de comillas\", \"Fpt\": \"Signo de puntuaci√≥n: cierre de comillas\", \"Fs\": \"Signo de puntuaci√≥n: punto y coma\", \"Ft\": \"Signo de puntuaci√≥n: signo de interrogaci√≥n\", \"Fx\": \"Signo de puntuaci√≥n: barra\", \"Fz\": \"Signo de puntuaci√≥n: otros\", \"I\": \"Interjecci√≥n\", \"NC\": \"Sustantivo com√∫n\", \"NP\": \"Nombre propio\", \"P0\": \"Pronombre personal\", \"PD\": \"Pronombre demostrativo\", \"PI\": \"Pronombre indefinido\", \"PN\": \"Pronombre numeral\", \"PP\": \"Pronombre posesivo\", \"PR\": \"Pronombre relativo\", \"PT\": \"Pronombre interrogativo\", \"PX\": \"Pronombre exclamativo\", \"RG\": \"Adverbio general\", \"RN\": \"Adverbio negativo\", \"SP\": \"Preposici√≥n\", \"VAI\": \"Verbo auxiliar infinitivo\", \"VAM\": \"Verbo auxiliar imperativo\", \"VAN\": \"Verbo auxiliar indicativo\", \"VAP\": \"Verbo auxiliar participio\", \"VAS\": \"Verbo auxiliar subjuntivo\", \"VMG\": \"Verbo principal gerundio\", \"VMI\": \"Verbo principal indicativo\", \"VMM\": \"Verbo principal imperativo\", \"VMN\": \"Verbo principal infinitivo\", \"VMP\": \"Verbo principal participio\", \"VMS\": \"Verbo principal subjuntivo\", \"VSG\": \"Verbo ser/estar gerundio\", \"VSI\": \"Verbo ser/estar indicativo\", \"VSM\": \"Verbo ser/estar imperativo\", \"VSN\": \"Verbo ser/estar infinitivo\", \"VSP\": \"Verbo ser/estar participio\", \"VSS\": \"Verbo ser/estar subjuntivo\", \"Y\": \"Conjunci√≥n\", \"Z\": \"N√∫mero\" }\n",
        "\n",
        "# Mostrar los resultados\n",
        "for token, pred in zip(tokens[1:-1], predicted_entities[1:-1]): # El primer y √∫ltimo token se omiten\n",
        "    pred = list(label_dict.keys())[pred-1]\n",
        "    entity = label_dict[pred]\n",
        "    print(f'{token:7s} [{pred}] {entity}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Ejemplo 3:** Detecci√≥n de idioma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este √∫ltimo ejemplo, vamos a utilizar el modelo **XLM-RoBERTa** dise√±ado para la tarea de detecci√≥n de idiomas.\n",
        "\n",
        "Fue entrenado en un conjunto de datos que incluye 20 lenguas, logrando una precisi√≥n promedio del 99.6% en un conjunto de Test.\n",
        "\n",
        "Este modelo se utiliza para detectar el idioma en el que se ha escrito un texto, muy util cuando se necesita analizar grandes vol√∫menes de datos multiling√ºes o cuando se trabaja con aplicaciones que requieren un procesamiento de lenguaje natural en diferentes idiomas.\n",
        "\n",
        "Puedes encontrar m√°s informaci√≥n y detalles sobre su uso [aqu√≠](https://huggingface.co/papluca/xlm-roberta-base-language-detection)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Cargar el tokenizador y el modelo\n",
        "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Textos a analizar\n",
        "texts = [\"Brevity is the soul of wit.\", \"Amor, ch'a nullo amato amar perdona.\", \"Hola, me llamo Jose\", \"This is a text with two idiomas a la vez, a ver que pasa\"]\n",
        "\n",
        "# Tokenizar el texto\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Obtener las predicciones del modelo\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Obtener la salida del modelo\n",
        "logits = outputs.logits\n",
        "probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "# Obtener la lista de posibles clases\n",
        "id2lang = model.config.id2label\n",
        "\n",
        "# Diccionario de idiomas con c√≥digos como claves\n",
        "language_dict = { 'ar': 'arabic', 'bg': 'bulgarian', 'de': 'german', 'el': 'modern greek', 'en': 'english', 'es': 'spanish', 'fr': 'french', 'hi': 'hindi', 'it': 'italian', 'ja': 'japanese', 'nl': 'dutch', 'pl': 'polish', 'pt': 'portuguese', 'ru': 'russian', 'sw': 'swahili', 'th': 'thai', 'tr': 'turkish', 'ur': 'urdu', 'vi': 'vietnamese', 'zh': 'chinese' }\n",
        "\n",
        "for idx, text in enumerate(texts):\n",
        "    print(f\"{text}\")\n",
        "    \n",
        "    # Obtener las probabilidades y las clases m√°s probables\n",
        "    top_probs, top_classes = torch.topk(probabilities[idx], 2)\n",
        "\n",
        "    for i in range(top_classes.size(0)):  # Recorrer las dos clases m√°s probables\n",
        "        class_label = id2lang[top_classes[i].item()]\n",
        "        class_prob = top_probs[i].item()\n",
        "        print(f\"  ¬∑ [{language_dict[class_label].title()}] -> {class_prob*100:.1f} %\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3.2. - Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Utiliza el modelo https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli utilizando pipelines para obtener la compatibilidad de un texto con varias etiquetas posibles (Zero-shot classification).\n",
        "\n",
        "Utiliza `text_2` e intenta buscar su compatibilidad con las siguientes etiquetas:\n",
        "\n",
        "```python \n",
        "candidate_labels = [\"cultura\", \"sociedad\", \"econom√≠a\", \"salud\", \"deportes\", \"educaci√≥n\"]\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
