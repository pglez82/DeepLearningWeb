{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# 4.5 - Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En las próximas prácticas vamos a crear arquitecturas propias del aprendizaje no supervisado. Como ya sabrás, este tipo de problemas se caracterizan por la falta de etiquetas en los datos, lo que significa que no contamos con una respuesta correcta para cada entrada.\n",
        "\n",
        "En su lugar, buscamos descubrir patrones, estructuras y relaciones dentro de los datos. Una de las redes neuronales profundas más utilizadas para este tipo de problemas son los **Autoencoders**.\n",
        "\n",
        "Un Autoencoder es un tipo de red neuronal utilizada para aprender representaciones eficientes (codificaciones) de los datos, típicamente para la reducción de dimensionalidad.\n",
        "\n",
        "Durante su entrenamiento tienen como objetivo reconstruir su entrada en la salida, pasando la información a través de una red de capas más pequeñas.\n",
        "\n",
        "### Estructura de un Autoencoder\n",
        "\n",
        "![image](https://i.imgur.com/0gIK8Gd.png)\n",
        "\n",
        "Estos constan de dos partes principales:\n",
        "\n",
        "1. **Codificador (Encoder)**: Comprime la entrada a un espacio *latente* de menor dimensión que el espacio original.\n",
        "2. **Decodificador (Decoder)**: Reconstruye los datos originales a partir de la representación compacta creada por el codificador.\n",
        "\n",
        "En esta práctica entrenaremos un Autoencoder convolucional capaz de aprender una codificación en un espacio de 2D ($\\mathbb{R}^{2}$) para cada una de las imágenes de un conjunto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5.1 - Conjunto de datos\n",
        "\n",
        "Utilizaremos el conjunto [MNIST](https://es.wikipedia.org/wiki/Base_de_datos_MNIST) ya utilizado en prácticas anteriores. Como recordarás, este conjunto posee imágenes en escala de grises con números del 0 al 9 escritos a mano por personas. Cada una de estas imágenes tiene una resolución de $28 \\times 28$ píxels. Al ser en escala de grises, cada imagen será un tensor de $1 \\times 28 \\times 28$.\n",
        "\n",
        "Este conjunto de datos está pensado para resolver un problema de *Aprendizaje supervisado de multiclasificación*, pero en este caso descartaremos las etiquetas y utilizaremos solamente las imágenes.\n",
        "\n",
        "### Descargar conjunto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizaremos de nuevo  ``torchvision`` y su submódulo ``datasets`` para obtener el conjunto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Fijar la semilla para obtener reproducibilidad y crear variable device\n",
        "seed = 42\n",
        "torch.manual_seed(seed)  # Fijar semilla de PyTorch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Definimos una transformación que convierte las imágenes a tensores.\n",
        "# La transformación ToTensor() convierte una imagen PIL en un tensor de PyTorch.\n",
        "# El compose permite concatenar múltiples transformaciones, en este caso solo aplicamos una.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Descargamos el dataset indicando donde almacenarlo, la partición y las transformaciones\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Creamos un conjunto de validación, dado que no viene por defecto\n",
        "mnist_train, mnist_val = random_split(mnist_train, (0.8, 0.2))\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Tras descargar, ya tentemos un objeto Dataset, por lo que necesitamos un DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5.2 - Autoencoder Convolucional\n",
        "\n",
        "Los autoencoders no poseen una arquitectura definida; esta depende del problema y del tipo de datos con los que trabajemos. Lo único que definen es la necesidad de crear una parte *encoder* y otra *decoder*.\n",
        "\n",
        "Al trabajar con imágenes, nuestro encoder estará formado por capas convolucionales que procesarán la imagen y la proyectarán en un espacio de, en este caso, $\\mathbb{R}^{2}$. \n",
        "\n",
        "El decoder hará el proceso inverso: deberá generar, dado un vector 2D, una salida de $1 \\times 28 \\times 28$. Para llevar a cabo este proceso serán necesarias capas denominadas *deconvoluciones* o *convoluciones transpuestas*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Definir el autoencoder convolucional\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=2, padding=1),  # (batch, 8, 14, 14)\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(8, 16, 3, stride=2, padding=1),  # (batch, 16, 7, 7)\n",
        "            nn.Tanh(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # (batch, 32, 4, 4)\n",
        "            nn.Tanh(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 4 * 4, 64),  # Proyección a vector de 64D\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 2)  # Proyección a vector de 2D\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 32 * 4 * 4),\n",
        "            nn.Tanh(),\n",
        "            nn.Unflatten(1, (32, 4, 4)),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1),  # (batch, 16, 7, 7)\n",
        "            nn.Tanh(),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),  # (batch, 8, 14, 14)\n",
        "            nn.Tanh(),\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1),  # (batch, 1, 28, 28)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración de parámetros y carga del dataset MNIST\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Inicialización del modelo, criterio de pérdida y optimizador\n",
        "model = ConvAutoencoder()\n",
        "model.to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación entrenamos el modelo pasando como entrada y salida el mismo elemento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Entrenamiento del autoencoder\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        img, _ = data # Como se puede ver, no necesitamos las etiquetas en este caso\n",
        "        img = img.to(device)\n",
        "        output = model(img)\n",
        "        loss = criterion(output, img)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validación\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            img, _ = data # Como se puede ver, no necesitamos las etiquetas en este caso\n",
        "            img = img.to(device)\n",
        "            output = model(img)\n",
        "            val_loss += criterion(output, img).item()\n",
        "    \n",
        "    val_loss /= len(val_loader)\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} Validation Loss: {val_loss:.4f}')\n",
        "    \n",
        "    # Mostrar una imagen del conjunto de validación y su predicción\n",
        "    val_data_iter = iter(val_loader) \n",
        "    val_img, _ = next(val_data_iter)  # Obtener una imagen del conjunto de validación\n",
        "    val_img = val_img.to(device)\n",
        "    with torch.no_grad():\n",
        "        val_output = model(val_img)\n",
        "\n",
        "    # Visualización\n",
        "    plt.figure(figsize=(9, 3))\n",
        "    \n",
        "    # Imagen original\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Imagen Original\")\n",
        "    plt.imshow(val_img[0].cpu().squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Imagen reconstruida\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Predicción del Modelo\")\n",
        "    plt.imshow(val_output[0].cpu().squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4.3. - Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Modifica y reentrena el autoencoder para que proyecte a un vector en $\\mathbb{R}^{8}$ en vez de $\\mathbb{R}^{2}$. A continuación crea un modelo para resolver el problema de multi-clasficación MNIST utilizando como entrada, en vez de las imágenes, el vector aprendido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Compara la accuracy en test del modelo de multi-clasificación anterior con uno que utilice las imágenes como entrada."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
