{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L_PPIxkwH6c"
      },
      "source": [
        "## Optimización de hiperparámetros (Optuna)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pglez82/DeepLearningWeb/blob/master/labs/notebooks/Optimizaci%C3%B3n%20de%20hiperparámetros%20(Optuna).ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "En el entrenamiento de redes neuronales profundas, existen una multitud de hiperparámetros que podemos optimizar. Algunos de los más importantes son los siguientes:\n",
        "- Learning rate\n",
        "- Momento\n",
        "- Tamaño del mini-batch\n",
        "- Weight decay\n",
        "- Cantidad de dropout\n",
        "- Optimizador utilizado\n",
        "- Número de capas de la red\n",
        "- Tamaño de las capas de la red (u otros parámetros de la capa como tamaño del kernel para CNNs, etc)\n",
        "- Funciones de activación usadas\n",
        "- etc.\n",
        "\n",
        "Es muy común que la búsqueda de hiperparámetros sea hecha de manera bastante artesanal, siguiendo la intuición y los conocimientos del científico de datos. Aún así, existe software que nos permite hacer esta búsqueda de manera más **sistemática y automática**. Uno de estos software es Optuna, que es el que veremos en esta práctica. Wandb también tiene su sistema de búsqueda de hiperparámetros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAkiPfJ8wP_7"
      },
      "source": [
        "### Instalación de los paquetes necesarios\n",
        "Para esta práctica necesitaremos instalar Optuna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpBLl-HswWrZ"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqgWdgabwbki"
      },
      "source": [
        "### Definición de la red\n",
        "\n",
        "Para este ejemplo vamos a partir de la red de una práctica anterior (la usada para el conjunto Fashion MNIST)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE9voObKwbP7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, dropout=0.2, linear_sizes = (50, 50, 10)):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        previous_size = 28*28 # la entrada tienen que coincidir con el número de pixeles en la imagen\n",
        "        for _, linear_size in enumerate(linear_sizes):\n",
        "            self.layers.append(nn.Linear(previous_size, linear_size))\n",
        "            self.layers.append(nn.Dropout(dropout))\n",
        "            previous_size = linear_size\n",
        "        # La capa de salida siempre tiene que tener 10 neuronas (10 clases)\n",
        "        self.layers.append(nn.Linear(previous_size,10)) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7nHK8jewtxc"
      },
      "source": [
        "Como puedes ver, el constructor de esta red ya nos permite alterar el dropout y el número y tamaño de capas lineales sin tener que modificar nada. Esto va a ser muy útil para el uso de optuna."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcrDZmtKw-Rq"
      },
      "source": [
        "### Carga de datos y creación de los dataloders\n",
        "\n",
        "Optuna va a tratar de buscar los mejores hiperparámetros tratando de optimizar una métrica concreta. Esto dependerá del problema en particular. En este caso trataremos de **optimizar el acierto sobre el conjunto de validación**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzIJYZuYxNxe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(root=\"data\",train=True,download=True,transform=ToTensor())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Utilizando dispositivo: %s\" % device)\n",
        "\n",
        "print(\"Datos de entrenamiento:\")\n",
        "print(training_data, end='\\n\\n')\n",
        "\n",
        "# Separación de un conjunto de validación\n",
        "training_data, validation_data = random_split(training_data,(50000,10000))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(validation_data, batch_size=64, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuA4tNOA0gMt"
      },
      "source": [
        "### Definición de los bucles de entrenamiento y validación\n",
        "Estos bucles deben estar parametrizados para que cada entrenamiento use los hiperparámetros generados por Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwzEHiER0rkl"
      },
      "outputs": [],
      "source": [
        "def validation(model, loss_module, val_dataloader):\n",
        "    val_loss=0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data_inputs, data_labels in val_dataloader:\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "            logits = model(data_inputs)\n",
        "            val_loss += loss_module(logits, data_labels)\n",
        "        return val_loss/ len(val_dataloader)\n",
        "\n",
        "def train(train_dataloader, val_dataloader, dropout, linear_sizes, learning_rate, epoch_callback):\n",
        "    # Creamos la red con los parámetros indicados por Optuna\n",
        "    model = Net(dropout = dropout, linear_sizes = linear_sizes).to(device)\n",
        "\n",
        "    # Definimos la función de pérdida\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Definimos el optimizador\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    min_loss = float('inf')\n",
        "    patience = 3\n",
        "    no_improvement = 0\n",
        "    # Training loop\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for data_inputs, data_labels in train_dataloader:\n",
        "            #Hacer una pasada hacia delante\n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)  # Output is [Batch size, 1], but we want [Batch size]\n",
        "            #Calcular el valor de la función de pérdida para este mini-batch\n",
        "            loss = loss_module(preds, data_labels)\n",
        "            #Acumular el error (solo para luego mostrarlo)\n",
        "            epoch_loss += loss.item()\n",
        "            #Reiniciar los gradientes\n",
        "            optimizer.zero_grad()\n",
        "            #Pasada hacia atrás\n",
        "            loss.backward()\n",
        "            #Actualizar los parámetros\n",
        "            optimizer.step()\n",
        "        val_loss = validation(model, loss_module, val_dataloader)\n",
        "        epoch_callback(val_loss, epoch)\n",
        "        print(\"[Epoch %d] Training Loss %0.2f. Validation Loss %0.2f. Patience: %d/%d\" % (epoch, epoch_loss/len(train_dataloader), val_loss, no_improvement, patience))\n",
        "        if val_loss < min_loss:\n",
        "            min_loss = val_loss\n",
        "            no_improvement=0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        # parada temprana\n",
        "        if no_improvement>=patience:\n",
        "            return min_loss\n",
        "    return min_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_KUaXHyx9XF"
      },
      "source": [
        "### Creación de la función objetivo\n",
        "La función objetivo es la función que Optuna va a ejecutar en cada intento. En estos intentos Optuna calculará un conjunto de hiperparámetros basado en los hiperparámetros usados en intentos anteriores. Ten en cuenta que podríamos hacer una búsqueda exhaustiva de parámetros (estilo a un grid search), pero lo normal es hacer una búsqueda con algún tipo de heurístico que facilite la búsqueda de los mejores hiperparámetros. Luego hablaremos más de este tema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GIUKKFSyg8S"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # learning rate\n",
        "    lr = trial.suggest_float(\"lr\", 0.00001, 0.01, log=True)\n",
        "\n",
        "    # Número y tamaño de las capas lineales\n",
        "    num_linear_layers = trial.suggest_int(\"number_of_layers\", 1, 2)\n",
        "    linear_sizes=[]\n",
        "    for i in range(num_linear_layers):\n",
        "        linear_sizes.append(trial.suggest_int(\"linear_sizes{}\".format(i), 1, 100))\n",
        "\n",
        "    dropout = trial.suggest_float(\"dropout\", 0, 0.5)\n",
        "    #batch_size = trial.suggest_int(\"batch_size\", 2, 64)\n",
        "\n",
        "    parameters = {'lr': lr, 'num_linear_layers':num_linear_layers, 'linear_sizes': linear_sizes, 'dropout':dropout}\n",
        "\n",
        "    print(\"Empezando un nuevo intento con los siguientes parámetros:\",parameters)\n",
        "\n",
        "    #Esta función se llama al final de cada época y sirve para podar las ejecuciones menos prometedoras\n",
        "    def epoch_callback(loss, epoch):\n",
        "        trial.report(loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return train(train_dataloader, val_dataloader, dropout, linear_sizes, lr, epoch_callback)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDG8tqL718cW"
      },
      "source": [
        "### Lanzamiento de la búsqueda de hiperparámetros\n",
        "\n",
        "En este caso vamos a utilizar para podar las ejecuciones menos prometedoras el `HyperbandPruner`. Puedes consultar la [documentación](https://optuna.readthedocs.io/en/stable/) de Optuna para ver otras opciones.\n",
        "\n",
        "Por otro lado, la búsqueda de hiperparámetros utiliza una base de datos para almacenar toda la información sobre la misma (ejecuciones, errores por época, parámetros ya probados, etc). Esto permite además poder parar y volver a lanzar el proceso ya que la búsqueda se persiste en esta base de datos y por tanto las ejecuciones y su resultados no se pierden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzReAbOZ2A4G"
      },
      "outputs": [],
      "source": [
        "from optuna.pruners import HyperbandPruner\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "pruner = HyperbandPruner()\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    study_name=\"Fashion_mnist\",\n",
        "    storage=\"sqlite:///busqueda_hiperparametros.db\",\n",
        "    load_if_exists=True,\n",
        "    pruner=pruner,\n",
        ")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Estadísticas del estudio: \")\n",
        "print(\"  Intentos satisfactorios: \", len(study.trials))\n",
        "print(\"  Intentos podados: \", len(pruned_trials))\n",
        "print(\"  Intentos completos: \", len(complete_trials))\n",
        "\n",
        "print(\"Mejor intento:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Valor: \", trial.value)\n",
        "\n",
        "print(\"  Hiperparámetros: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smVlpGli4QFZ"
      },
      "source": [
        "### Monitorización del proceso\n",
        "Una herramienta muy útil para ver el proceso de Optuna es optuna-dashboard. Esta herramienta abre un interfaz web donde podemos ver el progreso de la búsqueda de hiperparámetros. Para lanzarla, necesitamos instalarla y dar acceso a la misma a la base de datos donde se guarda la información relativa a la búsqueda. En Google Colab, puede hacerse de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkqInYBm4iuC"
      },
      "outputs": [],
      "source": [
        "!pip install optuna-dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nota: este código solo es válido para Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOc4p4m_4q9y"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import threading\n",
        "from optuna_dashboard import wsgi\n",
        "import optuna\n",
        "from wsgiref.simple_server import make_server\n",
        "\n",
        "\n",
        "port = 1234\n",
        "storage = optuna.storages.RDBStorage(\"sqlite:////content/busqueda_hiperparametros.db\")\n",
        "app = wsgi(storage)\n",
        "httpd = make_server(\"localhost\", port, app)\n",
        "thread = threading.Thread(target=httpd.serve_forever)\n",
        "thread.start()\n",
        "time.sleep(3) # Wait until the server startup\n",
        "\n",
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(port, path='/dashboard/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_wy2sGyJOCW"
      },
      "source": [
        "#### Para abrir optuna-dashboard en local\n",
        "En local simplemente puedes ejecutar en una terminal:\n",
        "`optuna-dashboard sqlite:///busqueda_hiperparametros.db`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbv0xU1iJOCW"
      },
      "source": [
        "#### Ejercicios\n",
        "\n",
        "1. Añade el optimizador como un hiperparámetro más y prueba SGD, Adam y AdamW.\n",
        "2. Convierte este notebook a un script y prepáralo para realizar esta búsqueda de parámetros en el servidor utilizando SLURM. Deja la tarea encolada y luego revisa los resultados."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
