{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# 4.1. - Redes GRU y LSTM\n",
        "\n",
        "En este notebook, exploraremos dos tipos de Redes Neuronales Recurrentes (RNN): las Long Short-Term Memory (LSTM) y las Gated Recurrent Unit (GRU). Ambas están diseñadas para manejar **secuencias de datos** y resolver el problema del **desvanecimiento del gradiente** de las redes recurrentes básicas.\n",
        "\n",
        "## 4.1.1. - Repaso de teoría\n",
        "\n",
        "### Redes LSTM\n",
        "\n",
        "Las LSTM fueron introducidas por Hochreiter y Schmidhuber en 1997. La clave de las LSTM es la celda de memoria, que permite a la red recordar valores por largos periodos de tiempo.\n",
        "\n",
        "#### Estructura de una LSTM\n",
        "\n",
        "Una LSTM consta de una celda de memoria, una puerta de entrada, una puerta de salida y una puerta de olvido. Cada una de estas puertas tiene una función específica para controlar el flujo de información dentro de la celda de memoria.\n",
        "\n",
        "### Redes GRU\n",
        "\n",
        "Las GRU fueron introducidas por Cho et al. en 2014. Son una variante simplificada de las LSTM, con menos puertas. Consta de dos puertas principales: una puerta de actualización y una puerta de reinicio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkE49jCZAk6"
      },
      "source": [
        "## 4.1.2. - Conjunto de datos\n",
        "Para este ejemplo, utilizaremos un pequeño conjunto de datos de texto (La venganza de Don Mendo) para entrenar los modelos GRU y LSTM. La tarea consistirá en predecir la siguiente palabra en una secuencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZfCuC476ZJ_m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "import re\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Descargar el texto de \"La venganza de Don Mendo\" de Pedro Muñoz Seca\n",
        "url = \"https://www.gutenberg.org/cache/epub/49013/pg49013.txt\"\n",
        "response = requests.get(url, timeout=30)\n",
        "text = response.text\n",
        "\n",
        "# Preprocesar el texto\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "words = text.split()\n",
        "\n",
        "# Crear diccionarios de palabras a índices y viceversa\n",
        "word_counts = Counter(words)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Crear el dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, words, word_to_ix, seq_length):\n",
        "        self.data = []\n",
        "        self.seq_length = seq_length\n",
        "        for i in range(len(words) - seq_length):\n",
        "            seq_in = words[i:i + seq_length]\n",
        "            seq_out = words[i + seq_length]\n",
        "            self.data.append((seq_in, seq_out))\n",
        "        self.word_to_ix = word_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_in, seq_out = self.data[idx]\n",
        "        seq_in = torch.tensor([self.word_to_ix[word] for word in seq_in], dtype=torch.long)\n",
        "        seq_out = torch.tensor(self.word_to_ix[seq_out], dtype=torch.long)\n",
        "        return seq_in, seq_out\n",
        "\n",
        "seq_length = 5\n",
        "dataset = TextDataset(words, word_to_ix, seq_length)\n",
        "\n",
        "# Dividir el dataset en entrenamiento, validación y prueba\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el peso para cada palabra\n",
        "total_words = len(words)\n",
        "word_freq = np.array([word_counts[word] for word in vocab])\n",
        "inverse_freq = total_words / (word_freq + 1e-10)  # Añadir un pequeño valor para evitar división por cero\n",
        "weights = np.max(inverse_freq) / inverse_freq  # Normalizar los pesos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPfm07dlakr0"
      },
      "source": [
        "## 4.1.3. - Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cLfuYHTqYyY-"
      },
      "outputs": [],
      "source": [
        "# Definir el modelo GRU\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = self.embedding(x)\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = self.embedding(x)\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, (h, c)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
        "                torch.zeros(1, batch_size, self.hidden_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Definir la función de pérdida con pesos\n",
        "class WeightedCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super(WeightedCrossEntropyLoss, self).__init__()\n",
        "        self.weights = weights\n",
        "    \n",
        "    def forward(self, outputs, targets):\n",
        "        # Calcular la pérdida cruzada con los pesos\n",
        "        return F.cross_entropy(outputs, targets, weight=self.weights)\n",
        "    \n",
        "# Convertir los pesos a un tensor de PyTorch\n",
        "weights = torch.tensor(weights, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qF7BumDnXe8O"
      },
      "outputs": [],
      "source": [
        "# Parámetros del modelo\n",
        "embed_size = 256\n",
        "hidden_size = 128\n",
        "output_size = vocab_size\n",
        "num_epochs = 20\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Inicializar los modelos, loss function y optimizer\n",
        "gru_model = GRUModel(vocab_size, embed_size, hidden_size, output_size)\n",
        "lstm_model = LSTMModel(vocab_size, embed_size, hidden_size, output_size)\n",
        "criterion = WeightedCrossEntropyLoss(weights) # nn.CrossEntropyLoss()\n",
        "gru_optimizer = optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
        "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3al3i38a-fG"
      },
      "source": [
        "## 4.1.4. - Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMuEf7NbXBKu",
        "outputId": "0a1ef0f0-95a0-405a-98cc-c012c44bb5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando modelo GRU...\n",
            "Epoch [1/20], Train Loss: 8.3835, Val Loss: 8.1756\n",
            "Epoch [2/20], Train Loss: 7.4496, Val Loss: 5.9779\n",
            "Epoch [3/20], Train Loss: 4.2070, Val Loss: 3.4668\n",
            "Epoch [4/20], Train Loss: 3.2983, Val Loss: 3.3115\n",
            "Epoch [5/20], Train Loss: 3.2039, Val Loss: 3.2717\n",
            "Epoch [6/20], Train Loss: 3.1723, Val Loss: 3.2433\n",
            "Epoch [7/20], Train Loss: 3.1298, Val Loss: 3.2194\n",
            "Epoch [8/20], Train Loss: 3.0962, Val Loss: 3.1986\n",
            "Epoch [9/20], Train Loss: 3.0664, Val Loss: 3.1760\n",
            "Epoch [10/20], Train Loss: 3.0342, Val Loss: 3.1561\n",
            "Epoch [11/20], Train Loss: 3.0015, Val Loss: 3.1359\n",
            "Epoch [12/20], Train Loss: 2.9683, Val Loss: 3.1158\n",
            "Epoch [13/20], Train Loss: 2.9268, Val Loss: 3.0968\n",
            "Epoch [14/20], Train Loss: 2.8994, Val Loss: 3.0787\n",
            "Epoch [15/20], Train Loss: 2.8576, Val Loss: 3.0600\n",
            "Epoch [16/20], Train Loss: 2.8214, Val Loss: 3.0436\n",
            "Epoch [17/20], Train Loss: 2.7862, Val Loss: 3.0284\n",
            "Epoch [18/20], Train Loss: 2.7466, Val Loss: 3.0135\n",
            "Epoch [19/20], Train Loss: 2.7129, Val Loss: 3.0020\n",
            "Epoch [20/20], Train Loss: 2.6789, Val Loss: 2.9908\n",
            "Entrenando modelo LSTM...\n",
            "Epoch [1/20], Train Loss: 8.4139, Val Loss: 8.2984\n",
            "Epoch [2/20], Train Loss: 7.7279, Val Loss: 6.0580\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo LSTM\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenando modelo LSTM...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[15], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m model(inputs, h)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\Pablo\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Pablo\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Pablo\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Función de entrenamiento y validación\n",
        "def train_model(model, optimizer, train_loader, val_loader, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_train_loss = 0\n",
        "        total_val_loss = 0\n",
        "\n",
        "        # Entrenamiento\n",
        "        for inputs, targets in train_loader:\n",
        "            h = model.init_hidden(inputs.size(0))\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs, h)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validación\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                h = model.init_hidden(inputs.size(0))\n",
        "                outputs, _ = model(inputs, h)\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_val_loss += loss.item()\n",
        "        model.train()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {total_train_loss/len(train_loader):.4f}, '\n",
        "              f'Val Loss: {total_val_loss/len(val_loader):.4f}')\n",
        "\n",
        "# Entrenar el modelo GRU\n",
        "print(\"Entrenando modelo GRU...\")\n",
        "train_model(gru_model, gru_optimizer, train_loader, val_loader, num_epochs)\n",
        "\n",
        "# Entrenar el modelo LSTM\n",
        "print(\"Entrenando modelo LSTM...\")\n",
        "train_model(lstm_model, lstm_optimizer, train_loader, val_loader, num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sCHWTAcbIsK",
        "outputId": "d6e59635-3403-4687-86fe-624d0f96a703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicción con GRU:\n",
            "si desenvaino el acero vais de la de la de la de la de la\n",
            "Predicción con LSTM:\n",
            "si desenvaino el acero vais de de de de de de de de de de\n"
          ]
        }
      ],
      "source": [
        "# Función de predicción\n",
        "def predict(model, word_to_ix, ix_to_word, start_words, predict_len):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([word_to_ix[word] for word in start_words], dtype=torch.long).unsqueeze(0)\n",
        "    h = model.init_hidden(input_seq.size(0))\n",
        "    predicted_words = start_words\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        output, h = model(input_seq, h)\n",
        "        _, top_idx = torch.max(output, 1)\n",
        "        predicted_word = ix_to_word[top_idx.item()]\n",
        "        predicted_words.append(predicted_word)\n",
        "        input_seq = torch.cat((input_seq[:, 1:], top_idx.unsqueeze(0)), 1)\n",
        "\n",
        "    return ' '.join(predicted_words)\n",
        "\n",
        "# Predecir con el modelo GRU\n",
        "print(\"Predicción con GRU:\")\n",
        "print(predict(gru_model, word_to_ix, ix_to_word, [\"si\", \"desenvaino\", \"el\", \"acero\", \"vais\"], 10))\n",
        "\n",
        "# Predecir con el modelo LSTM\n",
        "print(\"Predicción con LSTM:\")\n",
        "print(predict(lstm_model, word_to_ix, ix_to_word, [\"si\", \"desenvaino\", \"el\", \"acero\", \"vais\"], 10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
