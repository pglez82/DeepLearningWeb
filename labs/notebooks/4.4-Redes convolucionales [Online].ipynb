{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# 4.4 - Redes convolucionales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este notebook, exploraremos cómo resolver un problema de **clasificación de imágenes** utilizando una red convolucional (CNN).\n",
        "\n",
        "Como sabrás, estas redes se componen de capas convolucionales que son especialmente efectivas para procesar datos con una estructura de cuadrícula, como las imágenes.\n",
        "\n",
        "Las redes convolucionales aplican filtros para extraer características relevantes de las imágenes y, a través de operaciones como la convolución y el pooling, pueden detectar patrones locales que son invariables a la traslación.\n",
        "\n",
        "Para crear este tipo de redes, PyTorch nos proporciona, [entre otras](https://pytorch.org/docs/stable/nn.html#convolution-layers), las siguientes capas: \n",
        "* **`torch.nn.Conv1d`**: Capa que aplica una Convolución 1D.\n",
        "* **`torch.nn.Conv2d`**: Capa que aplica una Convolución 2D.\n",
        "* **`torch.nn.Conv3d`**: Capa que aplica una Convolución 3D.\n",
        "\n",
        "Además de las capas convolucionales, también necesitamos [otras capas](https://pytorch.org/docs/stable/nn.html#pooling-layers) y funciones para construir una CNN completa:\n",
        "\n",
        "* **`torch.nn.MaxPool2d`**: Capa que aplica una operación de Max Pooling 2D. También existen versiones 1D y 3D, así como con otras funciones como la media (AvgPool).\n",
        "* **`torch.nn.Flatten`**: Capa que aplana la entrada, convirtiéndola en una dimensión.\n",
        "* **`torch.nn.AdaptiveAvgPool2d`**: Capa que aplica una operación de Global Average Pooling 2D. Como sucedía en el pooling normal, existen varias versiones.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "Utilizaremos el conjunto [MNIST](https://es.wikipedia.org/wiki/Base_de_datos_MNIST) ya utilizado en prácticas anteriores. \n",
        "\n",
        "![image.png](https://i.imgur.com/wR4qw0l.png)\n",
        "\n",
        "Como puedes ver en la figura, este conjunto posee imágenes en escala de grises con números del 0 al 9 escritos a mano por personas.\n",
        "\n",
        "Cada una de estas imágenes tiene una resolución de 28x28 píxels. Al ser en escala de grises, cada imagen será un tensor de 1x28x28.\n",
        "\n",
        "Crearemos, por tanto una red que, a partir de una imagen del conjunto, intente predecir el dígito que aparece reflejado en esta.\n",
        "\n",
        "\n",
        "### Descargar conjunto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La librería ``torchvision`` posee multiples herramientas que facilitan la resolución de problemas con imágenes.\n",
        "\n",
        "Una de estas herramientas es la submódulo ``datasets``, que incluye conjuntos de datos populares y utilidades para cargarlos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Fijar la semilla para obtener reproducibilidad y crear variable device\n",
        "seed = 42\n",
        "torch.manual_seed(seed)  # Fijar semilla de PyTorch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Definimos una transformación que convierte las imágenes a tensores.\n",
        "# La transformación ToTensor() convierte una imagen PIL en un tensor de PyTorch.\n",
        "# El compose permite concatenar múltiples transformaciones, en este caso solo aplicamos una.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Descargamos el dataset indicando donde almacenarlo, la partición y las transformaciones\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Creamos un conjunto de validación, dado que no viene por defecto\n",
        "mnist_train, mnist_val = random_split(mnist_train, (0.8, 0.2))\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Tras descargar, ya tentemos un objeto Dataset, por lo que necesitamos un DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis de datos\n",
        "\n",
        "Para este problema, vamos a visualizar algunos de los ejemplos así como comprobar cada una de sus dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear una figura con 5 varias imágenes del conjunto\n",
        "plt.figure(figsize=(10, 2))\n",
        "# Para extraer ejemplos de un DataLoader, necesitamos un iterador\n",
        "iterator = iter(train_loader)\n",
        "# Pedimos el siguiente batch al iterador (y nos dará batch_size ejemplos)\n",
        "image, label = next(iterator)\n",
        "# Creamos un gráfico para mostrar batch_size ejemplos\n",
        "for i in range(batch_size):\n",
        "    plt.subplot(1, batch_size, i + 1)\n",
        "    plt.imshow(image[i][0].squeeze(), cmap='gray')\n",
        "    plt.title(f\"{label[i].item()}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos a ver el tamaño de una de las imágenes\n",
        "print(image[i].size())\n",
        "# Y su rango de valores\n",
        "print(image[i].max(), image[i].min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como puedes ver, este conjunto ya viene normalizado en el rango [0, 1]. \n",
        "\n",
        "Como sabrás, típicamente las imágenes se mueven en el rango [0-255], el cual habría que normalizar o estandarizar antes de introducir al modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4.2. - Modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un modelo muy sencillo con las siguientes capas:\n",
        "* Convolucion 2D con 3 filtros de 3x3.\n",
        "* Función de activación ReLU.\n",
        "* MaxPooling2D.\n",
        "* Capa Flatten para transformar el volumen de salida de la capa anterior en un vector.\n",
        "* Capa lineal de salida tamaño 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3, padding=1, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3 * 14 * 14, 10)  # 32 canales, 14x14 tamaño de imagen después de la capa de pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)  # Aplicamos convolución, ReLU y max pooling\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "    \n",
        "learning_rate = 0.0005\n",
        "num_epochs = 5\n",
        "\n",
        "# Creamos el modelo\n",
        "model = SimpleCNN()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Recuerda que ya aplica la softmax\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Imprimir la arquitectura\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los tamaños de las entradas, salidas y número de parámetros de las redes convolucionales son más difíciles de ver, para entender mejor lo que pasa en nuestro modelo vamos a utilizar la librería ``torchsummary``.\n",
        "\n",
        "Para instalarla:\n",
        "``` shell\n",
        "pip install torch-summary\n",
        "\n",
        "```\n",
        "\n",
        "Utilizarla es muy sencillo, solo tenemos que dar al método `summary` el modelo y el tamaño que tendrá un ejemplo de nuestro conjunto.\n",
        "\n",
        "Como resultado veremos una lista con cada una de las capas del modelo, el volumen que producen en la salida (output shape) y el número de parámetros que se aprenden en ella. \n",
        "\n",
        "También veremos un resumen de todos los parámetros del modelo y cuales de ellos se pueden entrenar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entrenar el modelo, podemos utilizar los métodos creados en prácticas anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "# Función para calcular la precisión (accuracy) en un conjunto de datos\n",
        "def calculate_accuracy(model, data_loader, K=1):\n",
        "    model.eval()  # Configuramos el modelo en modo evaluación (no se actualizan los pesos)\n",
        "    correct = 0  # Contador para predicciones correctas\n",
        "    total = 0  # Contador para el número total de ejemplos\n",
        "\n",
        "    with torch.no_grad():  # Desactivamos el cálculo de gradientes\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)  # Hacemos una predicción con el modelo\n",
        "            # Obtenemos las probabilidades de las clases (Top-K)\n",
        "            topk = outputs.topk(K, dim=1)[1]\n",
        "            # Creamos una máscara que indica si la etiqueta correcta está entre las top-K predicciones\n",
        "            correct_mask = topk.eq(targets.view(-1, 1).expand_as(topk))\n",
        "            correct += correct_mask.sum().item()  # Contamos las predicciones correctas\n",
        "            total += targets.size(0)  # Contamos el número total de ejemplos\n",
        "\n",
        "    accuracy = correct / total  # Calculamos la precisión como el porcentaje de predicciones correctas\n",
        "    return accuracy\n",
        "\n",
        "# Función de entrenamiento y validación\n",
        "def train_model(model, optimizer, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()  # Registramos el tiempo de inicio de la epoch\n",
        "\n",
        "        total_train_loss = 0  # Inicializamos la variable para acumular la pérdida de entrenamiento\n",
        "        total_val_loss = 0    # Inicializamos la variable para acumular la pérdida de validación\n",
        "\n",
        "        # Entrenamiento\n",
        "        model.train() # Configuramos el modelo en modo entrenamiento\n",
        "        for inputs, targets in train_loader:  # Iteramos sobre los datos de entrenamiento\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()  # Reseteamos los gradientes del optimizador\n",
        "            outputs = model(inputs)  # Hacemos una predicción con el modelo\n",
        "            loss = criterion(outputs, targets)  # Calculamos la pérdida entre la predicción y el objetivo\n",
        "            loss.backward()  # Calculamos los gradientes (backpropagation)\n",
        "            optimizer.step()  # Actualizamos los pesos del modelo según el optimizador\n",
        "            total_train_loss += loss.item()  # Acumulamos la pérdida de esta iteración\n",
        "\n",
        "        # Calculamos la precisión para el conjunto de entrenamiento\n",
        "        train_accuracy = calculate_accuracy(model, train_loader)\n",
        "        \n",
        "        # Validación\n",
        "        model.eval()  # Configuramos el modelo en modo evaluación (no se actualizan los pesos)\n",
        "        with torch.no_grad():  # Desactivamos el cálculo de gradientes para ahorrar memoria y mejorar la velocidad\n",
        "            for inputs, targets in val_loader:  # Iteramos sobre los datos de validación\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)  # Hacemos una predicción con el modelo\n",
        "                loss = criterion(outputs, targets)  # Calculamos la pérdida para validación\n",
        "                total_val_loss += loss.item()  # Acumulamos la pérdida de esta iteración\n",
        "\n",
        "        # Calculamos la precisión para el conjunto de validación\n",
        "        val_accuracy = calculate_accuracy(model, val_loader)\n",
        "        val_accuracy_3 = calculate_accuracy(model, val_loader, K=3)\n",
        "\n",
        "        # Calculamos el tiempo total de la epoch\n",
        "        epoch_time = time.time() - start_time  \n",
        "\n",
        "        # Imprimimos las estadísticas de la epoch: pérdida de entrenamiento, validación, precisión y tiempo\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {total_train_loss/len(train_loader):.4f}, '\n",
        "              f'Val Loss: {total_val_loss/len(val_loader):.4f}, '\n",
        "              f'Train T1 Accuracy: {train_accuracy:.4f}, '\n",
        "              f'Val T1 Accuracy: {val_accuracy:.4f}, '\n",
        "              f'Val T3 Accuracy: {val_accuracy_3:.4f}, '\n",
        "              f'Time: {epoch_time:.2f} sec')\n",
        "\n",
        "# Entrenar el modelo CNN\n",
        "print(f\"Entrenando CNN en {device}...\")\n",
        "train_model(model, optimizer, train_loader, val_loader, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4.3. - Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Crea el código necesario para dibujar un batch de imágenes de test junto con su predicción (incluida la probabilidad)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Utiliza [este modelo](https://huggingface.co/farleyknight-org-username/vit-base-mnist) de HugginFace entrenado para la misma tarea y verifica su funcionamiento con la primera imagen de test.\n",
        "\n",
        "> OJO, si utilizas pipelines, igual tienes que transformar una imagen de tensor a imagen PIL con este método:\n",
        "\n",
        "```python\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "image_PIL = to_pil_image(image_tensor)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Crea, entrena y evalúa un modelo capaz de resolver el problema [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "```python\n",
        "# Descarga del dataset\n",
        "dataset_train = datasets.CIFAR10(...)\n",
        "# Un vector con los nombres de las clases\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
