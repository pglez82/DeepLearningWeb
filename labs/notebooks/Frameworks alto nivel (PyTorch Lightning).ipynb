{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frameworks de alto nivel (PyTorch Lightning)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://github.com/pglez82/DeepLearningWeb/blob/master/labs/notebooks/Frameworks%20alto%20nivel%20(PyTorch%20Lightning).ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "PyTorch Lightning es una biblioteca de código abierto diseñada para simplificar y acelerar el proceso de entrenamiento de modelos de aprendizaje profundo utilizando PyTorch. Proporciona una abstracción de alto nivel sobre PyTorch que ayuda a los investigadores y desarrolladores a escribir código más limpio y legible, al tiempo que aprovechan al máximo la flexibilidad y potencia de PyTorch.\n",
    "\n",
    "Una de las principales ventajas de PyTorch Lightning es su enfoque en la modularidad y la organización del código. Proporciona una estructura clara y consistente para definir modelos, optimizadores, funciones de pérdida y bucles de entrenamiento y validación. Esto facilita la reutilización de código y la experimentación rápida con diferentes arquitecturas y configuraciones de entrenamiento.\n",
    "\n",
    "Además, PyTorch Lightning se encarga de tareas como la configuración de dispositivos (CPU o GPU), la administración automática de la memoria, el seguimiento de métricas durante el entrenamiento y la gestión de la generación de registros (logs). También incluye características avanzadas, como el entrenamiento distribuido y la integración con bibliotecas populares de visualización y registro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning==2.0.1\n",
    "!pip install ipympl # para gráficos interactivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la red\n",
    "En este caso vamos a entrenar un pequeño **autoencoder** para ejemplificar el uso de PyTorch Lightning. En lugar de utilizar como clase base la clase `torch.nn.Module`, utilizaremos `pl.LightningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "import lightning.pytorch as pl\n",
    "import torchmetrics as tm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definimos la red\n",
    "class AutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "        self.decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "        # Utilizado para acumular el loss en el bucle de validación\n",
    "        self.validation_loss = tm.MeanSquaredError()\n",
    "\n",
    "    # Este método define el bucle de entrenamiento\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #Al ser un autoencoder, no necesitamos la y\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.validation_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss = self.validation_loss.compute()\n",
    "        self.log(\"val_loss_epoch\", val_loss)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "model = AutoEncoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos el Dataset\n",
    "En este caso utilizaremos la clase que PyTorch Lightning provee para la carga de datos: `pl.LightningDataModule`. Para ello debemos de implementar una serie de métodos que devolverán los DataLoaders correspondientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=2):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # descargamos los datos\n",
    "        MNIST(root='data', train=True, download=True)\n",
    "        MNIST(root='data', train=False, download=True)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = MNIST(root='data', train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [50000, 10000])\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_test = MNIST(root='data', train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "mnist = MNISTDataModule(batch_size=128, num_workers=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando el modelo\n",
    "Para entrenar el modelo solo necesitaremos crear una instancia de la clase `Trainer` y llamar al método `fit`. ESta clase nos ecapsula, entre otras cosas, las siguientes características:\n",
    "\n",
    "\n",
    "- Bucle de entrenamiento\n",
    "- Las llamadas `optimizer.step()`, `loss.backward()`, `optimizer.zero_grad()`.\n",
    "- Las llamadas `model.eval()`, para deshabilitar los gradientes durante la evaluación.\n",
    "- Carga y salvado de checkpoints.\n",
    "- Logueo del experimento (usando tensorboard)\n",
    "- Entrenamiento en múltiples GPUs y TPUs\n",
    "- Entrenamiento con precisión de 16 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por defecto utilizará todas las gpus disponibles\n",
    "trainer = pl.Trainer(max_epochs=5, accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n",
    "trainer.fit(model=model, datamodule=mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia\n",
    "Una vez que hemos entrenado nuestro autoencoder vamos a aplicarlo a los datos de test. La idea es imprimir las representaciones en el espacio latente de estas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "latent_space_embeddings = trainer.predict(model, datamodule=mnist)\n",
    "latent_space_embeddings = torch.concat(latent_space_embeddings, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrando los resultados\n",
    "\n",
    "En este caso vamos a usar como método de representación un gráfico en 3D para poder visulizar los cluster formados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Visualizar las representaciones en el espacio latente\n",
    "scatter = ax.scatter(\n",
    "    latent_space_embeddings[:, 0],\n",
    "    latent_space_embeddings[:, 1],\n",
    "    latent_space_embeddings[:, 2],\n",
    "    c=mnist.mnist_test.targets,\n",
    "    cmap='tab10'\n",
    ")\n",
    "\n",
    "# Añadir los colores\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Etiquetas para los ejes\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimensión 2')\n",
    "ax.set_zlabel('Dimensión 3')\n",
    "\n",
    "# Mostrar el plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios propuestos\n",
    "1. Prueba a entrenar el autoencoder con el conjunto FashionMNIST. ¿Consigues los mismos resultados?\n",
    "2. Ejecuta este notebook en Google Colab en un entorno con GPU. Prueba en el Trainer a cambiar el acelerados y observa como de manera transparente puedes cambiar de dispositivo de entrenamiento.\n",
    "3. Observa el directorio lighning_logs y analiza la inforamción que hay en él.\n",
    "4. Prueba este mismo ejemplo usando Fashion-MNIST. ¿Qué resultados obtienes?\n",
    "5. Crea un script para realizar un entrenamiento básico utilizando MNIST para clasificación. Prueba tu modelo entrenado con PyTorch Lightning utilizando los datos de test. ¿Te resulta más o menos fácil que hacerlo con PyTorch directamente?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
