{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a PyTorch (Parte 3)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pglez82/DeepLearningWeb/blob/master/labs/notebooks/Introducci%C3%B3n%20a%20PyTorch%20(Parte%203).ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "En esta tercera parte vamos a crear una pequeña red neuronal con PyTorch para resolver nuestro primer problema de clasificación. Como veremos, trabajaremos con tensores pero también con modulos ya programados en PyTorch que representan capas de una red neuronal. La clase básica para hacer esto es torch.nn.Module. Vamos a ir haciendolo paso por paso.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generación y preparación del dataset\n",
    "Lo primero vamos a generar unos datos. Queremos generar un dataset con cierta complejidad, que no sea **linearmente separable**. Como hemos visto en teoría un buen ejemplo de esto es el problema **XOR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "#Necesario para que funcionen los plots en google colab\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(2032)\n",
    "\n",
    "# Función para generar un dataset estilo XOR\n",
    "def generate_xor_dataset(n_samples):\n",
    "    \n",
    "    X = np.random.rand(n_samples, 2)\n",
    "    y = np.logical_xor(X[:, 0] > 0.5, X[:, 1] > 0.5)\n",
    "    y = np.where(y, 1, 0)\n",
    "    return X, y\n",
    "\n",
    "# Generar dataset\n",
    "X, y = generate_xor_dataset(n_samples=200)\n",
    "\n",
    "# Plot el dataset\n",
    "data_0 = X[y == 0]\n",
    "data_1 = X[y == 1]\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(data_0[:, 0], data_0[:, 1], edgecolor=\"#333\", label=\"Clase 0\")\n",
    "plt.scatter(data_1[:, 0], data_1[:, 1], edgecolor=\"#333\", label=\"Clase 1\")\n",
    "plt.title(\"XOR dataset\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que en este caso he decidido generar los datos y visualizarlos usando Numpy, pero como ya sabemos podemos convertirlos de manera muy sencilla a tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.float32(X))\n",
    "y = torch.from_numpy(y)\n",
    "print(\"Dimension X\", X.shape)\n",
    "print(\"Dimension y\", y.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch necesita que los datos estén en una clase que herede de **torch.data.Dataset**. Aquí tenemos muchas variantes que iremos viendo a lo largo del curso. Si tenemos ya los datos en formato numérico en un par de tensores (como es el caso), podemos usar TensorDataset. Para imágenes existen clases ya programadas. Incluso para datasets muy utilizados como ImageNet, podemos utilizar la clase **torchvision.datasets.ImageNet**. Todas ellas heredan de la misma clase e implementan los mismos métodos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X,y)\n",
    "print(\"Número de ejemplos: \",len(dataset)) #Tenemos 200 ejemplos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes imaginar un Dataset en PyTorch simplemente como una clase que es capaz de devolver un único ejemplo, tenga la forma que tenga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí hemos obtenido el primer ejemplo, que en este caso pertenece a la clase positiva."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando entrenamos redes neuronales, generalmente los datos se pasan en lotes. Para ello, debemos de ser capaces de cargar múltiples ejemplos de una vez en un mini-batch. La clase que se encarga de hacer esto de manera eficiente en Pyorch es el **Dataloader**, que siempre recibe como parámetro un dataset del que cargar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True) #suffle true indica que los ejemplos vienen en orden aleatorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver por ejemplo como el DataLoader devolvería ejemplos de 8 en 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs, data_labels = next(iter(data_loader))\n",
    "print(\"Mini-batch X\", data_inputs.shape, \"\\n\", data_inputs)\n",
    "print(\"Mini-batch y\", data_labels.shape, \"\\n\", data_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiendo la arquitectura de la red\n",
    "Una vez que tenemos los datos preparados, vamos a definir la arquitectura de la red. En este caso vamos a optar por una red totalmente conectada, simple, con una capa oculta y una función de activación ReLU en la capa oculta y sigmoide en la de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        # Inicializar los módulos para la red\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn_hidden = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.act_fn_out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Definir como se hace una pasada hacia delante\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn_hidden(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act_fn_out(x)\n",
    "        return x\n",
    "    \n",
    "model = MLP(num_inputs=2, num_hidden=10, num_outputs=1)\n",
    "# Imprimir el modelo\n",
    "print(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la estructura del modelo imprimiéndolo. También sería posible iterar sobre los parámetros aprendibles de la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parámetro {name}, forma {param.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de pérdida\n",
    "Como ya sabemos, elegir la función de pérdida es crítico. En este caso es un problema de clasificación binario así que utilizaremos la función BCE (Binary Cross Entropy Loss). Esta función ya viene programa por defecto en PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizador\n",
    "Como ya sabemos, también debemos elegir un optimizador. Utilizaremos el más básico que es Stochastic-Gradient Descent. Tendremos que escoger también el **learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenando la red\n",
    "Vamos ahora a codificar el bucle general de entrenamiento de una red. Aquí lo que haremos será iterar sobre el dataloader, ir cargando mini-batches y haciendo una pasada adelante seguida de una pasada hacia atrás para calcular los gradientes. Por último, con la ayuda del optimizador, actualizaremos los pesos de la red para seguir iterando de esta manera un número de épocas determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "def train_model(model, optimizer, data_loader, loss_module, num_epochs=1000):\n",
    "    # Ponemos el modelo en modo train\n",
    "    model.train()\n",
    "\n",
    "    # Training loop\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = 0\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            #Hacer una pasada hacia delante\n",
    "            preds = model(data_inputs)\n",
    "            # Output es [Batch size, 1], pero queremos [Batch size]\n",
    "            preds = preds.squeeze(dim=1)  \n",
    "            #Calcular el valor de la función de pérdida para este mini-batch\n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "            #Acumular el error (solo para luego mostrarlo)\n",
    "            epoch_loss += loss.item()\n",
    "            #Reiniciar los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            #Pasada hacia atrás\n",
    "            loss.backward()\n",
    "            #Actualizar los parámetros\n",
    "            optimizer.step()\n",
    "        #Imprimir el error cada 50 épocas\n",
    "        if epoch%50==0:\n",
    "            pbar.set_description(\"Loss %0.2f\" % epoch_loss)\n",
    "\n",
    "train_model(model, optimizer, data_loader, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "def visualize_classification(model, data, label):\n",
    "    data_0 = data[label == 0]\n",
    "    data_1 = data[label == 1]\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(data_0[:, 0], data_0[:, 1], edgecolor=\"#333\", label=\"Clase 0\")\n",
    "    plt.scatter(data_1[:, 0], data_1[:, 1], edgecolor=\"#333\", label=\"Clase 1\")\n",
    "    plt.title(\"XOR dataset\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.legend()\n",
    "\n",
    "    c0 = to_rgba(\"C0\")\n",
    "    c1 = to_rgba(\"C1\")\n",
    "    x = np.arange(0, 1, step=0.01)\n",
    "    xx1, xx2 = np.meshgrid(x, x)\n",
    "    model_inputs = np.stack([xx1.flatten(), xx2.flatten()], axis=-1)\n",
    "    #Modelo en modo evaluación (inferencia)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.from_numpy(np.float32(model_inputs))).reshape(xx1.shape).cpu().detach().numpy()\n",
    "    output_image = (1 - preds)[:, :, np.newaxis] * c0 + preds[:, :, np.newaxis] * c1\n",
    "\n",
    "    plt.imshow(output_image, origin=\"lower\", extent=(0, 1, 0, 1))\n",
    "    plt.grid(False)\n",
    "\n",
    "visualize_classification(model, X.cpu().numpy(), y.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios propuestos\n",
    "\n",
    "1. Prueba a cambiar el número de épocas del entrenamiento. ¿Qué sucede?\n",
    "2. Prueba a quitar la función de activación.\n",
    "3. Prueba a cambiar el número de neuronas en la capa oculta.\n",
    "4. Modifica el código para que se pueda entrenar este modelo en la GPU. Para ello deberás de mover el modelo a la gpu (cuda:0) así como los tensores necesarios\n",
    "5. Calcula el acierto del modelo e imprímelo.\n",
    "6. Genera un conjunto de test con las mismas características y evalúalo con este mismo modelo. ¿Qué error obtienes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
