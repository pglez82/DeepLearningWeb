{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# 4.1. - Redes recurrentes\n",
        "\n",
        "En este notebook, exploraremos uno de los tipos de Redes Neuronales Recurrentes (RNN) más utilizados: las Long Short-Term Memory (LSTM).\n",
        "Estas están diseñadas para manejar **secuencias de datos** y resolver el problema del **desvanecimiento del gradiente** de las redes recurrentes básicas.\n",
        "\n",
        "Como ya hemos visto en teoría, en una LSTM propagamos entre cada etapa dos vectores diferentes, el de estado y la salida en si. Para obtener dichos vectores, necesitamos calcular el valor de la puerta de actualización, la de olvido y la de salida.\n",
        "\n",
        "Este proceso será transparente a nosotros dado que Pytorch ya nos proporciona una capa [`LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) y simplemente tendremos que preocuparnos de manejar la secuencia de entrada y la de salida. Además, gracias a los parámetros `num_layers` y `bidirectional` podremos crear redes mucho más complejas de forma sencilla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkE49jCZAk6"
      },
      "source": [
        "## Conjunto de datos\n",
        "Puesto que ahora trabajaremos con secuencias, para este ejemplo vamos a utilizar un conjunto de datos de texto. En este caso hemos optado por descargar el libro \"Trafalgar\" de Benito Pérez Galdós.\n",
        "\n",
        "Como hemos explicado en teoría, existen muchos tipos de problemas que podemos resolver con secuencias, pero en este caso nos centraremos en una de las más sencillas de implementar: predecir la siguiente palabra en una secuencia de 5 palabras dadas.\n",
        "\n",
        "### Descargar conjunto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Descargar el texto de \"Trafalgar\" de Benito Pérez Galdós\n",
        "url = \"https://www.gutenberg.org/cache/epub/16961/pg16961.txt\"\n",
        "response = requests.get(url, timeout=30)\n",
        "text = response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesar texto\n",
        "\n",
        "Normalmente los conjuntos de datos contienen errores o elementos no deseados, en esta parte los eliminaremos. Algunos ejemplos típicos son los números, los saltos de línea `\\n` o los tabuladores `\\t`. <br>\n",
        "Si abres la url del libro en tu navegador verás que al inicio y al final nos aparece un aviso de copyright que tendremos que eliminar.\n",
        "\n",
        "En este punto verás que también separamos el texto en frases haciendo uso de la librería de texto `nltk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Eliminar saltos de línea y retornos de carro\n",
        "text = re.sub(r'[\\n\\r]+', ' ', text)\n",
        "# Eliminamos cabecera y fin en inglés\n",
        "text = text.split(\"FIN DE TRAFALGAR\")[0]\n",
        "text = text.split(\" -I- \")[1]\n",
        "# Dividir el texto en frases utilizando la librería nltk\n",
        "sentences = sent_tokenize(text)\n",
        "# Eliminar acentos utilizando la librería unidecode\n",
        "sentences = [unidecode(s) for s in sentences]\n",
        "# Pasar a minúsculas y eliminar resto de caracteres extraños (dos puntos, punto y coma, números...)\n",
        "sentences = [re.sub(r'[^a-z\\s]', '', s.lower()) for s in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtener vocabulario\n",
        "\n",
        "Una vez tenemos todas las frases, para codificar cada una de las palabras del texto es necesario obtener el llamado `corpus` o `vocabulario`. Este nos sirve para saber que palabras únicas (sin repetición) aparecen en nuestro texto, lo cual es útil para asignar un índice a cada una. Como verás, en este caso asignamos el índice en función de la frecuencia de aparición, por tanto, la palabra más frecuente tendrá el índice más bajo.\n",
        "\n",
        "Como recordarás, nuestro objetivo es que, dado un texto de máximo 5 palabras, el modelo entrenado sea capaz de continuar la frase hasta que decida finalizarla. Para lograrlo tenemos que tener en cuenta varios escenarios:\n",
        "* ¿Y si nos dan menos de 5 palabras?\n",
        "* ¿Como hacemos para detectar que la palabra generada por el modelo es la última de la frase?\n",
        "* ¿Si nos dan una palabra que no existe en nuestro vocabulario? Una en otro idioma, por ejemplo.\n",
        "\n",
        "Para solventar estos problemas, vamos a añadir tres palabras nuevas o `tokens` a nuestro vocabulario:\n",
        "* **\\<pad\\>**: Token que representa *padding*, es decir, si nos dan menos de 5 palabras, rellenaremos las que falten con este padding.\n",
        "* **\\<eos\\>**: Token que representa *end-of-sequence*, es decir, fin de secuencia. Si el modelo predice este token, sabremos que ya se acabó la frase y podemos detener la generación.\n",
        "* **\\<unk\\>**: Token que representa *unknown*, es decir, desconocido. Si una palabra no aparece en el vocabulario, el modelo la codificará con este token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Crear una lista con todas las palabras del texto\n",
        "words = \" \".join(sentences).split()\n",
        "# Obtener el corpus del texto (todas las palabras que aparecen y su frecuencia)\n",
        "word_counts = Counter(words)\n",
        "# Filtrar palabras que aparecen menos de 5 veces, dado que no aportan mucho\n",
        "word_counts = {word: count for word, count in word_counts.items() if count >=5}\n",
        "# Ordenamos el corpus por frecuencia\n",
        "word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "# Extraemos solo las claves del diccionario anterior para crear el corpus\n",
        "vocab = list(word_counts.keys())\n",
        "# Añadimos tokens especiales <pad> (padding), <eos> (end of sequence), y <unk> (unknown)\n",
        "vocab = [\"<pad>\", \"<unk>\"] + vocab + [\"<eos>\"]\n",
        "# Generamos un diccionario (y su inverso) donde asignamos un id a cada palabra según su frecuencia.\n",
        "# La palabra más frecuente será la 1. El 0 lo dejamos para el padding\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Tamaño del vocabulario: {vocab_size}')\n",
        "print(f'Word to Index mapping: {word_to_ix}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis de datos\n",
        "En todos los problemas de aprendizaje, es muy importante hacer un análisis de los datos con los que vamos a trabajar, lo cual nos puede servir para diagnosticar o evitar futuros problemas.<br>\n",
        "En este caso realizamos un simple análisis de la frecuencia de las palabras. \n",
        "\n",
        "Como siempre suele suceder en estos casos, las palabras más comunes son las llamadas `stop-words`. Estas palabras son las que más solemos repetir en nuestros textos y las forman los artículos, las preposiciones, las conjunciones, ... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualización de las 20 palabras más comunes\n",
        "words_x = list(word_counts.keys())[:20]\n",
        "counts_y = list(word_counts.values())[:20]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(words_x, counts_y)\n",
        "plt.title('Palabras más comunes')\n",
        "plt.xlabel('Palabras')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear dataset Pytorch\n",
        "Una vez tenemos nuestro conjunto limpio y analizado, podemos crear los ejemplos con los que alimentaremos nuestro modelo. Como habíamos comentado queremos intentar completar una frase dadas, como máximo 5 palabras.<br>\n",
        "En la siguiente clase `SentenceDataset` crearemos los ejemplos a partir de las frases del texto descargado. Estos ejemplos han de reflejar todos los escenarios a los que luego queremos que nuestro modelo pueda enfrentarse.\n",
        "\n",
        "Por tanto, para cada frase, nuestro `Dataset` creará los siguientes ejemplos:\n",
        "* Frase: *Frase de ejemplo*\n",
        "    * [\\<pad\\>, \\<pad\\>, \\<pad\\>, \\<pad\\>, frase], de\n",
        "    * [\\<pad\\>, \\<pad\\>, \\<pad\\>,  frase, de], ejemplo\n",
        "    * [\\<pad\\>, \\<pad\\>, frase,  de, ejemplo], \\<eos\\>\n",
        "\n",
        "El primer vector es la secuencia de entrada y el elemento del final es la salida deseada. Ten en cuenta que al modelo no le damos el texto, le daremos los índices de cada palabra.\n",
        "\n",
        "\n",
        "Si nuestro `word_to_ix` fuese este:\n",
        "```python\n",
        "word_to_ix = {\"<pad>\":0, \"frase\":1, \"de\":2, \"ejemplo\":3, \"<eos>\":4}\n",
        "```\n",
        "Nuestros ejemplos quedarían así:\n",
        "* [0, 0, 0, 0, 1], 2\n",
        "* [0, 0, 0, 1, 2], 3\n",
        "* [0, 0, 1, 2, 3], 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, seq_length):\n",
        "        self.data = []\n",
        "        self.seq_length = seq_length\n",
        "        self.word_to_ix = word_to_ix\n",
        "        \n",
        "        # El índice para <unk>\n",
        "        unk_index = self.word_to_ix[\"<unk>\"]\n",
        "        \n",
        "        # Para cada frase del conjunto de datos..\n",
        "        for sentence in sentences:\n",
        "            # Separamos la frase en una lista de palabras y al final se añadimos el token <eos>\n",
        "            words = sentence.split() + ['<eos>']\n",
        "            # Para cada palabra, creamos un ejemplo\n",
        "            for i in range(len(words)-1):\n",
        "                # Obtenemos las palabras de cada ejemplo\n",
        "                seq_in = words[max(0, i+1 - seq_length):i+1]\n",
        "                # Añadir padding si es necesario\n",
        "                seq_in = ['<pad>'] * (seq_length - len(seq_in)) + seq_in\n",
        "                # Obtenemos la salida deseada para la secuencia de entrada (la siguiente palabra)\n",
        "                seq_out = words[i+1]\n",
        "                # Convertir palabras a índices\n",
        "                seq_in_indices = [self.word_to_ix.get(word, unk_index) for word in seq_in]\n",
        "                seq_out_index = self.word_to_ix.get(seq_out, unk_index)\n",
        "                # Almacenamos el ejemplo solo si seq_out no es <unk> para evitar que el modelo prediga <unk>\n",
        "                if seq_out_index != unk_index:\n",
        "                    self.data.append((torch.tensor(seq_in_indices, dtype=torch.long), torch.tensor(seq_out_index, dtype=torch.long)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Obtenemos un ejemplo concreto (ya se han creado todos en el init)\n",
        "        seq_in, seq_out = self.data[idx]\n",
        "        return seq_in, seq_out\n",
        " \n",
        "seq_length = 5\n",
        "dataset = SentenceDataset(sentences, word_to_ix, seq_length)\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dividir el conjunto de datos\n",
        "Dividimos el conjunto al completo en ``entrenamiento``, ``validación`` y ``test`` (80%,10%,10%) de forma aleatoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fijar la semilla para obtener reproducibilidad\n",
        "seed = 42\n",
        "torch.manual_seed(seed)  # Semilla para PyTorch\n",
        "\n",
        "# Dividir el dataset en entrenamiento, validación y prueba\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPfm07dlakr0"
      },
      "source": [
        "## 4.1.3. - Modelo\n",
        "Para este problema crearemos el modelo más sencillo posible:\n",
        "* **Capa Embedding**: Para cada elemento de la entrada, aprenderá un embedding que lo representará en un espacio de dimensión `embed_size`.\n",
        "* **Capa LSTM**: Procesa el embedding de cada elemento de la secuencia de entrada, lo proyecta en otro espacio de tamaño `hidden_size` y retorna la secuencia de salida.\n",
        "    * Ojo, la capa LSTM retorna dos cosas, las salidas y los vectores de estado **en cada instante de tiempo**. En este caso ignoramos los estados y solo utilizamos la salida en el último instante.\n",
        "    * Como ya hemos comentado en teoría, en estas redes se acumula toda la información de la secuencia en el último elemento. \n",
        "* **Capa Linear**: Proyecta el vector del último elemento de la salida de la LSTM en un espacio de tamaño `output_size`. En este caso `output_size` es igual al tamaño del vocabulario dado que estamos intentando resolver un problema de multi-clasificación.\n",
        "\n",
        "Nótese que no aplicamos la función `softmax` dado que esta se aplica internamente en la `CrossEntropyLoss`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLfuYHTqYyY-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Definir el modelo LSTM\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, seq_length):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.embedding(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        # Usar solo la salida del último paso de tiempo\n",
        "        last_hidden = lstm_out[:, -1, :]\n",
        "        out = self.fc(last_hidden)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparámetros del modelo\n",
        "embed_size = 64\n",
        "hidden_size = 32\n",
        "output_size = vocab_size\n",
        "num_epochs = 50\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Inicializar los modelos, loss function y optimizer\n",
        "lstm_model = LSTMModel(vocab_size, embed_size, hidden_size, output_size, seq_length)\n",
        "lstm_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3al3i38a-fG"
      },
      "source": [
        "## 4.1.4. - Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** A continuación, crea el bucle de entrenamiento para este modelo utilizando los hiper-parámetros definidos previamente. Recuerda obtener también la loss de validación.\n",
        "\n",
        "> **EJERCICIO:** Obtén e imprime, al final de cada época, la accuracy del modelo en el conjunto de entrenamiento y validación. Utiliza la siguiente función: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para calcular la precisión (accuracy) en un conjunto de datos\n",
        "def calculate_accuracy(model, data_loader, K=1):\n",
        "    model.eval()  # Configuramos el modelo en modo evaluación (no se actualizan los pesos)\n",
        "    correct = 0  # Contador para predicciones correctas\n",
        "    total = 0  # Contador para el número total de ejemplos\n",
        "\n",
        "    with torch.no_grad():  # Desactivamos el cálculo de gradientes\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)  # Hacemos una predicción con el modelo\n",
        "            # Obtenemos las probabilidades de las clases (Top-K)\n",
        "            topk = outputs.topk(K, dim=1)[1]\n",
        "            # Creamos una máscara que indica si la etiqueta correcta está entre las top-K predicciones\n",
        "            correct_mask = topk.eq(targets.view(-1, 1).expand_as(topk))\n",
        "            correct += correct_mask.sum().item()  # Contamos las predicciones correctas\n",
        "            total += targets.size(0)  # Contamos el número total de ejemplos\n",
        "\n",
        "    accuracy = correct / total  # Calculamos la precisión como el porcentaje de predicciones correctas\n",
        "    return accuracy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
