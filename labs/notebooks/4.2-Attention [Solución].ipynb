{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD11kuoOXGuR"
      },
      "source": [
        "# 4.2 - Mecanismos de atención\n",
        "\n",
        "En este notebook, exploraremos cómo resolver un problema de predicción de palabras utilizando mecanismos de self-attention en lugar de LSTMs.\n",
        "\n",
        "El mecanismo de self-attention permite que cada elemento de una secuencia preste atención a otros elementos de la misma secuencia. Esto es útil para capturar dependencias a **largo plazo** a diferencia de las redes recurrentes tradicionales.\n",
        "\n",
        "Además, a diferencia de las redes LSTM o GRU, estas son **altamente paralelizables** dado que permiten tratar cada elemento de la secuencia de forma individual.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkE49jCZAk6"
      },
      "source": [
        "## Conjunto de datos\n",
        "\n",
        "Utilizaremos el mismo conjunto de datos que en la práctica anterior: el libro \"Trafalgar\" de Benito Pérez Galdós. Nuestro objetivo será predecir la siguiente palabra en una secuencia de 5 palabras dadas.\n",
        "\n",
        "### Descargar conjunto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Descargar el texto de \"Trafalgar\" de Benito Pérez Galdós\n",
        "url = \"https://www.gutenberg.org/cache/epub/16961/pg16961.txt\"\n",
        "response = requests.get(url, timeout=30)\n",
        "text = response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesar texto\n",
        "\n",
        "Normalmente los conjuntos de datos contienen errores o elementos no deseados, en esta parte los eliminaremos. Algunos ejemplos típicos son los números, los saltos de línea `\\n` o los tabuladores `\\t`. <br>\n",
        "Si abres la url del libro en tu navegador verás que al inicio y al final nos aparece un aviso de copyright que tendremos que eliminar.\n",
        "\n",
        "En este punto verás que también separamos el texto en frases haciendo uso de la librería de texto `nltk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Eliminar saltos de línea y retornos de carro\n",
        "text = re.sub(r'[\\n\\r]+', ' ', text)\n",
        "# Eliminamos cabecera y fin en inglés\n",
        "text = text.split(\"FIN DE TRAFALGAR\")[0]\n",
        "text = text.split(\" -I- \")[1]\n",
        "# Dividir el texto en frases utilizando la librería nltk\n",
        "sentences = sent_tokenize(text)\n",
        "# Eliminar acentos utilizando la librería unidecode\n",
        "sentences = [unidecode(s) for s in sentences]\n",
        "# Pasar a minúsculas y eliminar resto de caracteres extraños (dos puntos, punto y coma, números...)\n",
        "sentences = [re.sub(r'[^a-z\\s]', '', s.lower()) for s in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtener vocabulario\n",
        "\n",
        "Una vez tenemos todas las frases, para codificar cada una de las palabras del texto es necesario obtener el llamado `corpus` o `vocabulario`. Este nos sirve para saber que palabras únicas (sin repetición) aparecen en nuestro texto, lo cual es útil para asignar un índice a cada una. Como verás, en este caso asignamos el índice en función de la frecuencia de aparición, por tanto, la palabra más frecuente tendrá el índice más bajo.\n",
        "\n",
        "Como recordarás, nuestro objetivo es que, dado un texto de máximo 5 palabras, el modelo entrenado sea capaz de continuar la frase hasta que decida finalizarla. Para lograrlo tenemos que tener en cuenta varios escenarios:\n",
        "* ¿Y si nos dan menos de 5 palabras?\n",
        "* ¿Como hacemos para detectar que la palabra generada por el modelo es la última de la frase?\n",
        "\n",
        "Para solventar estos problemas, vamos a añadir dos palabras nuevas o `tokens` nuevos a nuestro vocabulario:\n",
        "* **\\<pad\\>**: Token que representa *padding*, es decir, si nos dan menos de 5 palabras, rellenaremos las que falten con este padding.\n",
        "* **\\<eos\\>**: Token que representa *end-of-sequence*, es decir, fin de secuencia. Si el modelo predice este token, sabremos que ya se acabó la frase y podemos detener la generación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Crear una lista con todas las palabras del texto\n",
        "words = \" \".join(sentences).split()\n",
        "# Obtener el corpus del texto (todas las palabras que aparecen y su frecuencia)\n",
        "word_counts = Counter(words)\n",
        "# Filtrar palabras que aparecen menos de 5 veces, dado que no aportan mucho\n",
        "word_counts = {word: count for word, count in word_counts.items() if count >=5}\n",
        "# Ordenamos el corpus por frecuencia\n",
        "word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "# Extraemos solo las claves del diccionario anterior para crear el corpus\n",
        "vocab = list(word_counts.keys())\n",
        "# Añadimos tokens especiales <pad> (padding), <eos> (end of sequence), y <unk> (unknown)\n",
        "vocab = [\"<pad>\", \"<unk>\"] + vocab + [\"<eos>\"]\n",
        "# Generamos un diccionario (y su inverso) donde asignamos un id a cada palabra según su frecuencia.\n",
        "# La palabra más frecuente será la 1. El 0 lo dejamos para el padding\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Tamaño del vocabulario: {vocab_size}')\n",
        "print(f'Word to Index mapping: {word_to_ix}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis de datos\n",
        "En todos los problemas de aprendizaje, es muy importante hacer un análisis de los datos con los que vamos a trabajar, lo cual nos puede servir para diagnosticar o evitar futuros problemas.<br>\n",
        "En este caso realizamos un simple análisis de la frecuencia de las palabras. \n",
        "\n",
        "Como siempre suele suceder en estos casos, las palabras más comunes son las llamadas `stop-words`. Estas palabras son las que más solemos repetir en nuestros textos y las forman los artículos, las preposiciones, las conjunciones, ... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualización de las 20 palabras más comunes\n",
        "words_x = list(word_counts.keys())[:20]\n",
        "counts_y = list(word_counts.values())[:20]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(words_x, counts_y)\n",
        "plt.title('Palabras más comunes')\n",
        "plt.xlabel('Palabras')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear dataset Pytorch\n",
        "Una vez tenemos nuestro conjunto limpio y analizado, podemos crear los ejemplos con los que alimentaremos nuestro modelo. Como habíamos comentado queremos intentar completar una frase dadas, como máximo 5 palabras.<br>\n",
        "En la siguiente clase `SentenceDataset` crearemos los ejemplos a partir de las frases del texto descargado. Estos ejemplos han de reflejar todos los escenarios a los que luego queremos que nuestro modelo pueda enfrentarse.\n",
        "\n",
        "Por tanto, para cada frase, nuestro `Dataset` creará los siguientes ejemplos:\n",
        "* Frase: *Frase de ejemplo*\n",
        "    * [\\<pad\\>, \\<pad\\>, \\<pad\\>, \\<pad\\>, frase], de\n",
        "    * [\\<pad\\>, \\<pad\\>, \\<pad\\>,  frase, de], ejemplo\n",
        "    * [\\<pad\\>, \\<pad\\>, frase,  de, ejemplo], \\<eos\\>\n",
        "\n",
        "El primer vector es la secuencia de entrada y el elemento del final es la salida deseada. Ten en cuenta que al modelo no le damos el texto, le daremos los índices de cada palabra.\n",
        "\n",
        "\n",
        "Si nuestro `word_to_ix` fuese este:\n",
        "```python\n",
        "word_to_ix = {\"<pad>\":0, \"frase\":1, \"de\":2, \"ejemplo\":3, \"<eos>\":4}\n",
        "```\n",
        "Nuestros ejemplos quedarían así:\n",
        "* [0, 0, 0, 0, 1], 2\n",
        "* [0, 0, 0, 1, 2], 3\n",
        "* [0, 0, 1, 2, 3], 4\n",
        "\n",
        "> **Nota:** La attention soporta la entrada de máscaras que sirven para indicar que elementos de la entrada son irrelevantes (\\<pad\\>). Se ha modificado el método `__get_item__` para que, además de la x y la y, también retorne un vector con la máscara de cada ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, seq_length):\n",
        "        self.data = []\n",
        "        self.seq_length = seq_length\n",
        "        self.word_to_ix = word_to_ix\n",
        "        \n",
        "        # El índice para <unk>\n",
        "        unk_index = self.word_to_ix[\"<unk>\"]\n",
        "        \n",
        "        # Para cada frase del conjunto de datos..\n",
        "        for sentence in sentences:\n",
        "            # Separamos la frase en una lista de palabras y al final se añadimos el token <eos>\n",
        "            words = sentence.split() + ['<eos>']\n",
        "            # Para cada palabra, creamos un ejemplo\n",
        "            for i in range(len(words)-1):\n",
        "                # Obtenemos las palabras de cada ejemplo\n",
        "                seq_in = words[max(0, i+1 - seq_length):i+1]\n",
        "                # Añadir padding si es necesario\n",
        "                seq_in = ['<pad>'] * (seq_length - len(seq_in)) + seq_in\n",
        "                # Obtenemos la salida deseada para la secuencia de entrada (la siguiente palabra)\n",
        "                seq_out = words[i+1]\n",
        "                # Convertir palabras a índices\n",
        "                seq_in_indices = [self.word_to_ix.get(word, unk_index) for word in seq_in]\n",
        "                seq_out_index = self.word_to_ix.get(seq_out, unk_index)\n",
        "                \n",
        "                # Almacenamos el ejemplo solo si seq_out no es <unk>\n",
        "                if seq_out_index != unk_index:\n",
        "                    self.data.append((torch.tensor(seq_in_indices, dtype=torch.long), torch.tensor(seq_out_index, dtype=torch.long)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Obtenemos un ejemplo concreto (ya se han creado todos en el init)\n",
        "        seq_in, seq_out = self.data[idx]\n",
        "        # Crear la máscara de padding (Las posiciones de padding se marcan como True)\n",
        "        mask = seq_in == self.word_to_ix[\"<pad>\"]\n",
        "        return seq_in, seq_out, mask\n",
        " \n",
        "seq_length = 5\n",
        "dataset = SentenceDataset(sentences, word_to_ix, seq_length)\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dividir el conjunto de datos\n",
        "Dividimos el conjunto al completo en ``entrenamiento``, ``validación`` y ``test`` (80%,10%,10%) de forma aleatoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fijar la semilla para obtener reproducibilidad\n",
        "seed = 42\n",
        "torch.manual_seed(seed)  # Semilla para PyTorch\n",
        "\n",
        "# Dividir el dataset en entrenamiento, validación y prueba\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPfm07dlakr0"
      },
      "source": [
        "## 4.2.3. - Modelo\n",
        "Para este problema crearemos el modelo más sencillo posible:\n",
        "* **Capa Embedding**: Para cada elemento de la entrada, aprenderá un embedding que lo representará en un espacio de dimensión `embed_size`.\n",
        "* **Capa MultiheadAttention**: Esta capa permite al modelo enfocarse en diferentes partes de la secuencia de entrada simultáneamente. Cada \"cabeza\" en la atención múltiple (multi-head) puede capturar distintos patrones de relación entre los elementos de la secuencia.\n",
        "    * En la capa LSTM de la práctica anterior, el último elemento de la secuencia de salida contiene información de toda la secuencia, **en este caso no**.\n",
        "    * El número de heads se configura mediante `num_heads`.\n",
        "* **Capa Linear**: Proyecta el vector del último elemento de la salida de la LSTM en un espacio de tamaño `output_size`. En este caso `output_size` es igual al tamaño del vocabulario dado que estamos intentando resolver un problema de multi-clasificación.\n",
        "\n",
        "Nótese que no aplicamos la función `softmax` dado que esta se aplica internamente en la `CrossEntropyLoss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLfuYHTqYyY-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, seq_length):\n",
        "        super(SelfAttentionModel, self).__init__()\n",
        "        # Capa de embedding: Convierte los índices de las palabras en vectores densos de tamaño embed_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # Capa de atención múltiple (MultiheadAttention): Permite al modelo enfocarse en diferentes partes de la secuencia\n",
        "        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
        "        # Capa de flatten para aplanar la salida de la atención\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Capa lineal para proyección al espacio de salida\n",
        "        self.fc = nn.Linear(embed_size * seq_length, vocab_size)\n",
        "\n",
        "    def forward(self, sentence, mask=None, return_att_mtx=False):\n",
        "        # Aplicación de la capa de embedding para convertir las palabras en embeddings\n",
        "        embeds = self.embedding(sentence)\n",
        "        # Aplicamos la atención sobre la secuencia de embeddings (en este caso query, key y value son los mismos)\n",
        "        # La máscara se aplica a la capa de atención para controlar qué posiciones se deben ignorar (las <pad>)\n",
        "        attention_out, attention_weights = self.attention(embeds, embeds, embeds, key_padding_mask=mask)\n",
        "        # Aplanar la salida de la atención\n",
        "        flattened_out = self.flatten(attention_out)\n",
        "        # Proyectar al espacio de salida\n",
        "        out = self.fc(flattened_out)\n",
        "        \n",
        "        if return_att_mtx: \n",
        "            return out, attention_weights\n",
        "        else: \n",
        "            return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparámetros del modelo\n",
        "embed_size = 8\n",
        "num_heads = 1\n",
        "num_epochs = 25\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Inicializar los modelos, loss function y optimizer\n",
        "model = SelfAttentionModel(vocab_size, embed_size, num_heads, seq_length)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3al3i38a-fG"
      },
      "source": [
        "## 4.2.4. - Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para calcular la precisión (accuracy) en un conjunto de datos\n",
        "def calculate_accuracy(model, data_loader, K=1):\n",
        "    model.eval()  # Configuramos el modelo en modo evaluación (no se actualizan los pesos)\n",
        "    correct = 0  # Contador para predicciones correctas\n",
        "    total = 0  # Contador para el número total de ejemplos\n",
        "\n",
        "    with torch.no_grad():  # Desactivamos el cálculo de gradientes\n",
        "        for inputs, targets, masks in data_loader:\n",
        "            inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
        "            outputs = model(inputs, masks)  # Hacemos una predicción con el modelo\n",
        "            # Obtenemos las probabilidades de las clases (Top-K)\n",
        "            topk = outputs.topk(K, dim=1)[1]\n",
        "            # Creamos una máscara que indica si la etiqueta correcta está entre las top-K predicciones\n",
        "            correct_mask = topk.eq(targets.view(-1, 1).expand_as(topk))\n",
        "            correct += correct_mask.sum().item()  # Contamos las predicciones correctas\n",
        "            total += targets.size(0)  # Contamos el número total de ejemplos\n",
        "\n",
        "    accuracy = correct / total  # Calculamos la precisión como el porcentaje de predicciones correctas\n",
        "    return accuracy\n",
        "\n",
        "# Función de entrenamiento y validación\n",
        "def train_model(model, optimizer, train_loader, val_loader, num_epochs):\n",
        "    model.train()  # Configuramos el modelo en modo entrenamiento\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()  # Registramos el tiempo de inicio de la epoch\n",
        "\n",
        "        total_train_loss = 0  # Inicializamos la variable para acumular la pérdida de entrenamiento\n",
        "        total_val_loss = 0    # Inicializamos la variable para acumular la pérdida de validación\n",
        "\n",
        "        # Entrenamiento\n",
        "        model.train() # Configuramos el modelo en modo entrenamiento\n",
        "        for inputs, targets, masks in train_loader:  # Iteramos sobre los datos de entrenamiento\n",
        "            inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()  # Reseteamos los gradientes del optimizador\n",
        "            outputs = model(inputs, masks)  # Hacemos una predicción con el modelo\n",
        "            loss = criterion(outputs, targets)  # Calculamos la pérdida entre la predicción y el objetivo\n",
        "            loss.backward()  # Calculamos los gradientes (backpropagation)\n",
        "            optimizer.step()  # Actualizamos los pesos del modelo según el optimizador\n",
        "            total_train_loss += loss.item()  # Acumulamos la pérdida de esta iteración\n",
        "\n",
        "        # Calculamos la precisión para el conjunto de entrenamiento\n",
        "        train_accuracy = calculate_accuracy(model, train_loader)\n",
        "        \n",
        "        # Validación\n",
        "        model.eval()  # Configuramos el modelo en modo evaluación (no se actualizan los pesos)\n",
        "        with torch.no_grad():  # Desactivamos el cálculo de gradientes para ahorrar memoria y mejorar la velocidad\n",
        "            for inputs, targets, masks in val_loader:  # Iteramos sobre los datos de validación\n",
        "                inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
        "                outputs = model(inputs, masks)  # Hacemos una predicción con el modelo\n",
        "                loss = criterion(outputs, targets)  # Calculamos la pérdida para validación\n",
        "                total_val_loss += loss.item()  # Acumulamos la pérdida de esta iteración\n",
        "\n",
        "        # Calculamos la precisión para el conjunto de validación\n",
        "        val_accuracy = calculate_accuracy(model, val_loader)\n",
        "        val_accuracy_3 = calculate_accuracy(model, val_loader, K=3)\n",
        "\n",
        "        # Calculamos el tiempo total de la epoch\n",
        "        epoch_time = time.time() - start_time  \n",
        "\n",
        "        # Imprimimos las estadísticas de la epoch: pérdida de entrenamiento, validación, precisión y tiempo\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Train Loss: {total_train_loss/len(train_loader):.4f}, '\n",
        "              f'Val Loss: {total_val_loss/len(val_loader):.4f}, '\n",
        "              f'Train T1 Accuracy: {train_accuracy:.4f}, '\n",
        "              f'Val T1 Accuracy: {val_accuracy:.4f}, '\n",
        "              f'Val T3 Accuracy: {val_accuracy_3:.4f}, '\n",
        "              f'Time: {epoch_time:.2f} sec')\n",
        "\n",
        "# Entrenar el modelo Attention\n",
        "print(f\"Entrenando modelo Attention en {device}...\")\n",
        "train_model(model, optimizer, train_loader, val_loader, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2.5. - Prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sCHWTAcbIsK",
        "outputId": "d6e59635-3403-4687-86fe-624d0f96a703"
      },
      "outputs": [],
      "source": [
        "def predict(model, text, word_to_ix, ix_to_word, seq_length, max_len=20):\n",
        "    # Asegurarse de que el modelo esté en modo de evaluación\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocesar el texto de entrada\n",
        "    text = unidecode(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "\n",
        "    # Convertir palabras a índices\n",
        "    input_sequence = [word_to_ix.get(word, word_to_ix['<pad>']) for word in words]\n",
        "    input_sequence = input_sequence[-seq_length:]  # Tomar solo las últimas seq_length palabras\n",
        "    input_sequence = [0] * (seq_length - len(input_sequence)) + input_sequence\n",
        "\n",
        "    # Convertir la secuencia a tensor\n",
        "    input_tensor = torch.tensor([input_sequence], dtype=torch.long)\n",
        "\n",
        "    # Inicializar la lista de palabras generadas\n",
        "    generated_words = words\n",
        "    predicted_word = \"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while len(generated_words)<=max_len and predicted_word!='<eos>':\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            mask = input_tensor == word_to_ix['<pad>']\n",
        "            # Hacer una predicción usando el modelo\n",
        "            output = model(input_tensor, mask)\n",
        "            \n",
        "            # Obtener la palabra con la probabilidad más alta\n",
        "            _, predicted_index = torch.max(output, 1)\n",
        "            predicted_word = ix_to_word[predicted_index.item()]\n",
        "\n",
        "            # Añadir la palabra generada a la lista\n",
        "            generated_words.append(predicted_word)\n",
        "\n",
        "            # Actualizar la secuencia de entrada para la siguiente predicción\n",
        "            input_sequence = input_sequence[1:] + [predicted_index.item()]\n",
        "            input_tensor = torch.tensor([input_sequence], dtype=torch.long)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n",
        "# Ejemplo de texto de entrada\n",
        "input_text = \"La batalla de\"\n",
        "predicted_text = predict(model, input_text, word_to_ix, ix_to_word, seq_length)\n",
        "print(predicted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2.6. - Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** ¿Que sucede si no se eliminan los ejemplos que predicen \\<unk\\> del dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hay que eliminar la siguiente parte del generador de datos\n",
        "if seq_out_index != unk_index:\n",
        "    self.data.append((torch.tensor(seq_in_indices, dtype=torch.long), torch.tensor(seq_out_index, dtype=torch.long)))   \n",
        "#Esto hará que el modelo, probablemente prediga casi siempre unk, dado que la mayoría de ejemplos, al haber eliminado muchas palabras poco frecuentes, tengan como salida unk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Extrae y representa la matriz de atención del modelo para el texto: _El barco es muy antiguo_ .Puedes utilizar el siguiente código para visualizarla:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_att_mtx(att_mtx, text):\n",
        "    # Este método recibe la matriz de atención y un texto\n",
        "    words = text.split()\n",
        "    fig, ax = plt.subplots()\n",
        "    cax = ax.matshow(att_mtx, cmap='viridis')\n",
        "    fig.colorbar(cax)\n",
        "    \n",
        "    ax.set_xticks(torch.arange(len(words)))\n",
        "    ax.set_yticks(torch.arange(len(words)))\n",
        "    \n",
        "    ax.set_xticklabels(words, rotation=90)\n",
        "    ax.set_yticklabels(words)\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"El barco es muy antiguo\"\n",
        "# Asegurarse de que el modelo esté en modo de evaluación\n",
        "model.eval()\n",
        "# Preprocesar el texto de entrada\n",
        "text = unidecode(text).lower()\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "words = text.split()\n",
        "# Convertir palabras a índices\n",
        "input_sequence = [word_to_ix.get(word, word_to_ix['<pad>']) for word in words]\n",
        "input_sequence = input_sequence[-seq_length:]  # Tomar solo las últimas seq_length palabras\n",
        "input_sequence = [0] * (seq_length - len(input_sequence)) + input_sequence\n",
        "# Convertir la secuencia a tensor\n",
        "input_tensor = torch.tensor([input_sequence], dtype=torch.long)\n",
        "\n",
        "with torch.no_grad():\n",
        " input_tensor = input_tensor.to(device)\n",
        " # Hacer una predicción usando el modelo y pedir matriz de atención\n",
        " output, att_mtx = model(input_tensor, return_att_mtx=True)\n",
        " # Eliminar dimensiones innecesarias de la matriz\n",
        " att_mtx = att_mtx.squeeze().cpu().numpy()\n",
        " \n",
        "plot_att_mtx(att_mtx, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Crea un nuevo modelo, sin eliminar el existente, donde se aprendan representaciones diferentes para cada palabra: Una para las **queries**, otra para las **keys** y otra para los **values**. Entrena el modelo de nuevo y compáralo con el anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esto es una forma de hacerlo, otra sería aprendiendo 3 embeddings directamente para cada palabra de la entrada, el problema es que tendríamos que aprender muchos más pesos\n",
        "\n",
        "class SelfAttentionModel_v2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, seq_length):\n",
        "        super(SelfAttentionModel_v2, self).__init__()\n",
        "        # Capa de embedding: Convierte los índices de las palabras en vectores densos de tamaño embed_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        \n",
        "        # Capas de proyección para queries, keys y values\n",
        "        self.query_projection = nn.Linear(embed_size, embed_size)\n",
        "        self.key_projection = nn.Linear(embed_size, embed_size)\n",
        "        self.value_projection = nn.Linear(embed_size, embed_size)\n",
        "        \n",
        "        # Capa de atención múltiple (MultiheadAttention): Permite al modelo enfocarse en diferentes partes de la secuencia\n",
        "        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
        "        \n",
        "        # Capa de flatten para aplanar la salida de la atención\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        # Capa lineal para proyección al espacio de salida\n",
        "        self.fc = nn.Linear(embed_size * seq_length, vocab_size)\n",
        "\n",
        "    def forward(self, sentence, mask=None):\n",
        "        # Aplicación de la capa de embedding para convertir las palabras en embeddings\n",
        "        embeds = self.embedding(sentence)\n",
        "        \n",
        "        # Proyección de embeddings para queries, keys y values\n",
        "        queries = self.query_projection(embeds)\n",
        "        keys = self.key_projection(embeds)\n",
        "        values = self.value_projection(embeds)\n",
        "        \n",
        "        # Aplicamos la atención sobre la secuencia de embeddings\n",
        "        # La máscara se aplica a la capa de atención para controlar qué posiciones se deben ignorar (las <pad>)\n",
        "        attention_out, _ = self.attention(queries, keys, values, key_padding_mask=mask)\n",
        "        \n",
        "        # Aplanar la salida de la atención\n",
        "        flattened_out = self.flatten(attention_out)\n",
        "        \n",
        "        # Proyectar al espacio de salida\n",
        "        out = self.fc(flattened_out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EJERCICIO:** Adapta el modelo para que resuelva un problema de análisis de sentimientos. El objetivo será predecir, a partir de un texto (tweets en este caso), el tono o sentimiento que refleja (positivo, neutro o negativo). A continuación tienes el código necesario para obtener el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data_url = \"https://raw.githubusercontent.com/pablo-pnunez/datasets/master/texto/sentiment_analysis_twitter.csv\"\n",
        "twitter_data = pd.read_csv(data_url)\n",
        "tweets, tweets_sentiment = twitter_data[\"text\"].str.strip().values, twitter_data[\"sentiment\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OJO, hay que eliminar algún nan del conjunto descargado y alguna review que no tiene nada. Estas últimas hacen que la attention sea nan.\n",
        "\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "from unidecode import unidecode\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for idx, tweet in enumerate(tweets):\n",
        "    try:\n",
        "        # Eliminar saltos de línea y retornos de carro\n",
        "        tweet = re.sub(r'[\\n\\r]+', ' ', tweet)\n",
        "        # Eliminar acentos utilizando la librería unidecode\n",
        "        tweet = unidecode(tweet)\n",
        "        # Pasar a minúsculas y eliminar resto de caracteres extraños (dos puntos, punto y coma, números...)\n",
        "        tweet = re.sub(r'[^a-z\\s]', '', tweet.lower())\n",
        "        # Eliminamos más de un espacio\n",
        "        tweet = re.sub(r'(\\s)+', ' ', tweet)\n",
        "        # Filtrar secuencias de 1 palabra o menos\n",
        "        if len(tweet.split()) > 1:\n",
        "            x_data.append(tweet)\n",
        "            y_data.append(tweets_sentiment[idx])\n",
        "    except:\n",
        "        print(f\"Error con el ejemplo {idx}, no se añade.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Crear una lista con todas las palabras del texto\n",
        "words = \" \".join(x_data).split()\n",
        "# Obtener el corpus del texto (todas las palabras que aparecen y su frecuencia)\n",
        "word_counts = Counter(words)\n",
        "# Filtrar palabras que aparecen menos de 5 veces, dado que no aportan mucho\n",
        "word_counts = {word: count for word, count in word_counts.items() if count >=5}\n",
        "# Ordenamos el corpus por frecuencia\n",
        "word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "# Extraemos solo las claves del diccionario anterior para crear el corpus\n",
        "vocab = list(word_counts.keys())\n",
        "# Añadimos tokens especiales <pad> (padding) y <unk> (unknown)\n",
        "vocab = [\"<pad>\", \"<unk>\"] + vocab \n",
        "# Generamos un diccionario (y su inverso) donde asignamos un id a cada palabra según su frecuencia.\n",
        "# La palabra más frecuente será la 1. El 0 lo dejamos para el padding\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f'Tamaño del vocabulario: {vocab_size}')\n",
        "print(f'Word to Index mapping: {word_to_ix}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data, word_to_ix, seq_length):\n",
        "        self.data = []\n",
        "        self.seq_length = seq_length\n",
        "        self.word_to_ix = word_to_ix\n",
        "        \n",
        "        # Un diccionario para realizar el One-Hot en la salida\n",
        "        y_map = {\"positive\":0, \"neutral\":1, \"negative\":2}\n",
        "\n",
        "        # El índice para <unk>\n",
        "        unk_index = self.word_to_ix[\"<unk>\"]\n",
        "        \n",
        "        # Para cada frase del conjunto de datos..\n",
        "        for idx, sentence in enumerate(x_data):\n",
        "            # Separamos la frase en una lista de palabras\n",
        "            words = sentence.split()\n",
        "            # Creamos un vector con el padding necesario hasta llegar a seq_length\n",
        "            seq_in = ['<pad>'] * (seq_length - len(words)) \n",
        "            # Añadimos las palabras\n",
        "            seq_in += words\n",
        "            # Convertir palabras a índices\n",
        "            seq_in_indices = [self.word_to_ix.get(word, unk_index) for word in seq_in]\n",
        "            # Transformar la salida a One-Hot\n",
        "            out = y_map[y_data[idx]]\n",
        "            # Almacenamos el ejemplo\n",
        "            self.data.append((torch.tensor(seq_in_indices, dtype=torch.long), torch.tensor(out, dtype=torch.long)))\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Obtenemos un ejemplo concreto (ya se han creado todos en el init)\n",
        "        seq_in, seq_out = self.data[idx]\n",
        "        # Crear la máscara de padding (Las posiciones de padding se marcan como True)\n",
        "        mask = seq_in == self.word_to_ix[\"<pad>\"]\n",
        "        return seq_in, seq_out, mask\n",
        "\n",
        "# Obtener la longitud de la mayor secuencia\n",
        "max_seq_length = max([len(s.split(\" \")) for s in x_data])\n",
        "dataset = SentimentDataset(x_data, y_data, word_to_ix, max_seq_length)\n",
        "print(len(dataset))\n",
        "\n",
        "# Fijar la semilla para obtener reproducibilidad\n",
        "seed = 42\n",
        "torch.manual_seed(seed)  # Semilla para PyTorch\n",
        "\n",
        "# Dividir el dataset en entrenamiento, validación y prueba\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentSelfAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, seq_length):\n",
        "        super().__init__()\n",
        "        # Capa de embedding: Convierte los índices de las palabras en vectores densos de tamaño embed_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # Capa de atención múltiple (MultiheadAttention): Permite al modelo enfocarse en diferentes partes de la secuencia\n",
        "        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
        "        # Capa de flatten para aplanar la salida de la atención\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Capa lineal para proyección al espacio de salida\n",
        "        self.fc = nn.Linear(embed_size * seq_length, 3)\n",
        "\n",
        "    def forward(self, sentence, mask=None, return_att_mtx=False):\n",
        "        # Aplicación de la capa de embedding para convertir las palabras en embeddings\n",
        "        embeds = self.embedding(sentence)\n",
        "        # Aplicamos la atención sobre la secuencia de embeddings (en este caso query, key y value son los mismos)\n",
        "        # La máscara se aplica a la capa de atención para controlar qué posiciones se deben ignorar (las <pad>)\n",
        "        attention_out, attention_weights = self.attention(embeds, embeds, embeds, key_padding_mask=mask)\n",
        "        # Aplanar la salida de la atención\n",
        "        flattened_out = self.flatten(attention_out)\n",
        "        # Proyectar al espacio de salida\n",
        "        out = self.fc(flattened_out)\n",
        "        \n",
        "        if return_att_mtx: \n",
        "            return out, attention_weights\n",
        "        else: \n",
        "            return out\n",
        "        \n",
        "# Hiperparámetros del modelo\n",
        "embed_size = 6\n",
        "num_heads = 1\n",
        "num_epochs = 50\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Inicializar los modelos, loss function y optimizer\n",
        "model = SentimentSelfAttentionModel(vocab_size, embed_size, num_heads, max_seq_length)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Entrenar el modelo Attention\n",
        "print(f\"Entrenando modelo Attention en {device}...\")\n",
        "train_model(model, optimizer, train_loader, val_loader, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text, word_to_ix, seq_length):\n",
        "    # Eliminar saltos de línea y retornos de carro\n",
        "    text = re.sub(r'[\\n\\r]+', ' ', text)\n",
        "    # Eliminar acentos\n",
        "    text = unidecode(text)\n",
        "    # Pasar a minúsculas y eliminar resto de caracteres extraños\n",
        "    text = re.sub(r'[^a-z\\s]', '', text.lower())\n",
        "    # Eliminamos más de un espacio\n",
        "    text = re.sub(r'(\\s)+', ' ', text)\n",
        "    # Separamos en palabras\n",
        "    words = text.split()\n",
        "    # Creamos un vector con el padding necesario hasta llegar a seq_length\n",
        "    seq_in = ['<pad>'] * (seq_length - len(words)) + words\n",
        "    # Convertir palabras a índices\n",
        "    seq_in_indices = [word_to_ix.get(word, word_to_ix[\"<unk>\"]) for word in seq_in]\n",
        "    return torch.tensor(seq_in_indices, dtype=torch.long).unsqueeze(0)  # Añadimos dimensión de batch\n",
        "\n",
        "def predict(model, text, word_to_ix, seq_length, device):\n",
        "    model.eval()  # Configuramos el modelo en modo evaluación\n",
        "    with torch.no_grad():\n",
        "        # Preprocesar y codificar el texto\n",
        "        input_tensor = preprocess_text(text, word_to_ix, seq_length).to(device)\n",
        "        mask = input_tensor == word_to_ix[\"<pad>\"]\n",
        "        # Obtener la predicción\n",
        "        output = model(input_tensor, mask)\n",
        "        # Obtener la clase con la mayor probabilidad\n",
        "        _, predicted_class = torch.max(output, 1)\n",
        "        return predicted_class.item()\n",
        "\n",
        "# Ejemplo de uso\n",
        "text_to_predict = \"I see you\"\n",
        "predicted_class = predict(model, text_to_predict, word_to_ix,  max_seq_length, device)\n",
        "\n",
        "# Mapear la clase predicha a su etiqueta\n",
        "class_labels = {0: \"positive\", 1: \"neutral\", 2: \"negative\"}\n",
        "predicted_label = class_labels[predicted_class]\n",
        "\n",
        "print(f'Texto: \"{text_to_predict}\"')\n",
        "print(f'Predicción: {predicted_label}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
