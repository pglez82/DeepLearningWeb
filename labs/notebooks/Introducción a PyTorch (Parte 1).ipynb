{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a PyTorch (Parte 1)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pglez82/DeepLearningWeb/blob/master/labs/notebooks/Introducci%C3%B3n%20a%20PyTorch%20(Parte%201).ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "PyTorch es un framework de aprendizaje profundo desarrollado por Facebook, de **código abierto** y con contribuciones de miles de usuarios. Es una alternativa a otros frameworks como TensorFlow o MXNet. El lenguaje de programación utilizado por este framework es Python (aunque muchas de sus partes están programas en otros lenguajes como C++). En este tutorial, vamos a aprender los fundamentos de PyTorch para que puedas utilizarlo en el resto de prácticas.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primeros pasos\n",
    "Lo primero consiste en ver si tenemos PyTorch instalado y conocer su versión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si en la salida anterior ves **cpu** será que estás ejecutando una compilación de PyTorch solo con soporte para CPU y no GPU. Si por el contrario quieres ejecutar PyTorch en una máquina con GPU como Google Colab (o incluso tu propia máquina con GPU y Cuda instalado), la salida de este comando debería indicartelo. Ten en cuenta que la versión con GPU también soporta entrenamientos en la CPU (lo contrario no es cierto)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un aspecto importante en los experimentos que hagamos será la reproducibilidad de resultados. Establecemos una semilla para que todos los números aleatorios generados sean los mismos ejecución tras ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2032)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensores\n",
    "\n",
    "Los tensores son la pieza clave en cualquier framework de aprendizaje profundo. Son equivalentes a los arrays de Numpy pero tienen ciertas diferencias muy importantes:\n",
    "\n",
    "1. Los tensores **pueden moverse entre diferentes dispositivos**. Es decir, podemos tener un tensor en CPU y moverlo a la GPU y todos los cálculos realizados con este pasarán a realizarse en este dispositivo.\n",
    "2. Los tensores están preparados para diferenciar sobre ellos (calcular las derivadas parciales necesarias para aplicar descenso de gradiente). \n",
    "\n",
    "De todas maneras, una de las principales ventajas de PyTorch es que si sabemos operar con arrays de Numpy, cambiar a hacerlo con tensores será muy sencillo. Vamos a crear nuestro primer Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(3, 4)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aquí tenemos un tensor de 3x4 de números reales, inicializado alteatoriamente. Podemos mostrar su dimensión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible inicializar tensores con otros valores, como por ejemplo, ceros, unos o valores aleatorios entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.zeros(3,4))\n",
    "print(torch.ones(3,4))\n",
    "print(torch.rand(3,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de crear un tensor es hacerlo desde un array de numpy existente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_arr = np.array([[1, 2], [3, 4]])\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este case se pude ver que como el array era de números enteros, el tensor resultante mantiene este tipo. Siempre podemos ver el **tipo de los elementos de un tensor** con la siguiente instrucción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que **el tipo es crítico** ya que nuestra red va a requerir muchísimos parámetros que al final van a ser tensores y la memoria de nuestros dispositivos es limitada. Te recomiendo el siguiente [enlace](https://pytorch.org/docs/stable/tensors.html) para conocer los diferentes tipos y saber cuando ocupa cada uno en memoria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de crear tensores, muchas veces es interesante convertirlos de vuelta a Numpy. Podemos hacerlo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que la llamada cpu() lo que hace es mover el tensor a la cpu (si no está ya). Es importante hacer esta llamada porque para pasar el tensor a numpy tiene que estar en cpu primero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mover tensores de un dispositivo a otro podemos hacerlo de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "tensor = tensor.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obviamente para que esto funcione debemos tener PyTorch instalado con soporte para cuda. En este caso **cuda:0** indica que queremos mover el tensor a la primera GPU del sistema. Es importante tener en cuenta que `tensor.to` devuelve el tensor en el nuevo dispositivo por tanto debemos recordar guardarlo en una variable para posteriormente poder usarlo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones con tensores\n",
    "\n",
    "Existen multitud de operaciones que se pueden realizar con tensores. En el siguiente [enlace](https://pytorch.org/docs/stable/tensors.html#) tienes una descripción completa de todas las operaciones que se pueden realizar. Aquí vamos a describir las más básicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "suma = t1+t2\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(suma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ten en cuenta que esta operación crea un nuevo tensor en memoria. En PyTorch es posible también realizar **operaciones sobre los mismos tensores**, para no gastar espacio extra en memoria. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.add_(t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos otras operaciones disponibles pero en general, **las operaciones básicas que puedes hacer con Numpy también se pueden hacer con tensores**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(\"Resta:\")\n",
    "print(t1-t2)\n",
    "print(\"Multiplicación por un escalar:\")\n",
    "print(t1*3)\n",
    "print(\"Multiplicación de matrices elemento a elemento:\")\n",
    "print(t1*t2)\n",
    "print(\"Multiplicación de matrices normal:\")\n",
    "#Importante: estamos haciendo la transpuesta de t2 para poder multiplicarlas y que coincidan las dimesiones\n",
    "print(t1@t2.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cambio de la forma de un tensor\n",
    "\n",
    "En muchas ocasiones necesitamos cambiar la forma de un tensor, para luego poder operar con él correctamente. Para esto es muy adecuada la función **view**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arange crea un tensor con valores desde 0 hasta n-1\n",
    "t1 = torch.arange(10)\n",
    "print(t1)\n",
    "\n",
    "#digamos que queremos una matriz de 5x2\n",
    "print(t1.view(5,2))\n",
    "\n",
    "#También podemos inferir dimensiones\n",
    "print(t1.view(5,-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "es interesante entender que view devuelve un nuevo tensor pero que comparte la estructura interna con el tensor original, es decir, no estamos almacenando los datos de nuevo, sino simplemente **dándoles otra forma**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexado\n",
    "El indexado funciona de igual manera que en Numpy. Veamos algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.arange(12).view(3,4)\n",
    "print(t1)\n",
    "\n",
    "print(\"Solo la segunda fila (empieza a contar en cero):\")\n",
    "print(t1[1,:])\n",
    "print(\"Solo la última columna:\")\n",
    "print(t1[:,-1])\n",
    "print(\"Seleccionar el primer elemento de las dos primeras filas:\")\n",
    "print(t1[:2,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios propuestos\n",
    "1. Navega por la documentación de PyTorch (https://pytorch.org/docs/stable/torch.html) y busca un par de funciones interesantes para operar con tensores que no aparezcan en este notebook.\n",
    "2. Cambia el tipo de dispositivo de tu máquina en google colab y trata de ejecutar el código anterior en diferentes dispositivos. Usa el atributo del tensor `device` para conocer en que dispositivo se encuentra.\n",
    "3. Intenta realizar una operación con dos tensores que se encuentren en dispositivos diferentes. ¿Qué sucede en este caso?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
